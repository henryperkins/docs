## Source Code for my Autodocumentation Project

## file_handlers.py

```python
import os
import shutil
import logging
import aiofiles
import aiohttp
import json
import asyncio
from typing import Set, List, Dict, Any, Optional
from language_functions import get_handler
from language_functions.base_handler import BaseHandler
from utils import (
    is_binary,
    get_language,
    is_valid_extension,
    clean_unused_imports_async,
    format_with_black_async,
    run_flake8_async
)
from write_documentation_report import (
    generate_documentation_prompt,
    write_documentation_report,
    sanitize_filename,
    generate_table_of_contents
)

logger = logging.getLogger(__name__)

async def extract_code_structure(content: str, file_path: str, language: str, handler: BaseHandler) -> Optional[Dict[str, Any]]:
    """
    Asynchronously extracts the code structure from the given content using the specified handler.

    Args:
        content (str): The source code content.
        file_path (str): Path to the source file.
        language (str): Programming language of the source code.
        handler (BaseHandler): The handler object for the specific language.

    Returns:
        Optional[Dict[str, Any]]: A dictionary representing the code structure or None if extraction fails.
    """
    logger.debug(f"Extracting code structure for '{file_path}' (language: {language})")
    try:
        structure = await asyncio.to_thread(handler.extract_structure, content, file_path)
        if not structure:
            logger.warning(f"No structure extracted from '{file_path}'")
            return None
        return structure
    except Exception as e:
        logger.error(f"Error extracting structure from '{file_path}': {e}", exc_info=True)
        return None

async def backup_and_write_new_content(file_path: str, new_content: str) -> None:
    """
    Creates a backup of the original file and writes new content to it.

    Args:
        file_path (str): Path to the file to update.
        new_content (str): The new content to write to the file.
    """
    backup_path = f'{file_path}.bak'
    try:
        if os.path.exists(backup_path):
            os.remove(backup_path)
            logger.debug(f"Removed existing backup at '{backup_path}'.")
        await asyncio.to_thread(shutil.copy, file_path, backup_path)
        logger.debug(f"Backup created at '{backup_path}'.")
        async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
            await f.write(new_content)
        logger.info(f"Inserted documentation into '{file_path}'.")
    except Exception as e:
        logger.error(f"Error writing to '{file_path}': {e}", exc_info=True)
        # Attempt to restore from backup
        if os.path.exists(backup_path):
            try:
                await asyncio.to_thread(shutil.copy, backup_path, file_path)
                os.remove(backup_path)
                logger.info(f"Restored original file from backup for '{file_path}'.")
            except Exception as restore_error:
                logger.error(f"Failed to restore backup for '{file_path}': {restore_error}", exc_info=True)

async def fetch_documentation_rest(
    session: aiohttp.ClientSession,
    prompt: str,
    semaphore: asyncio.Semaphore,
    deployment_name: str,
    function_schema: Dict[str, Any],
    azure_api_key: str,
    azure_endpoint: str,
    azure_api_version: str,
    retry: int = 3
) -> Optional[Dict[str, Any]]:
    """
    Fetches documentation from the Azure OpenAI API using the provided prompt.

    Args:
        session (aiohttp.ClientSession): The HTTP session for making requests.
        prompt (str): The prompt to send to the API.
        semaphore (asyncio.Semaphore): Semaphore to limit concurrent API requests.
        deployment_name (str): The Azure OpenAI deployment name.
        function_schema (Dict[str, Any]): The schema defining functions.
        azure_api_key (str): The API key for Azure OpenAI.
        azure_endpoint (str): The endpoint URL for the Azure OpenAI service.
        azure_api_version (str): The API version to use.
        retry (int, optional): Number of retries for failed requests. Defaults to 3.

    Returns:
        Optional[Dict[str, Any]]: The documentation generated by the API or None if failed.
    """
    logger.debug(f"Fetching documentation using REST API for deployment: {deployment_name}")

    url = f"{azure_endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version={azure_api_version}"
    headers = {
        "Content-Type": "application/json",
        "api-key": azure_api_key,
    }

    for attempt in range(1, retry + 1):
        try:
            async with semaphore:
                async with session.post(url, headers=headers, json={
                    "messages": [{"role": "user", "content": prompt}],
                    "functions": function_schema["functions"],
                    "function_call": {"name": "generate_documentation"},
                }) as response:
                    if response.status == 200:
                        data = await response.json()
                        logger.debug(f"API Response: {data}")

                        if "choices" in data and len(data["choices"]) > 0:
                            choice = data["choices"][0]
                            message = choice["message"]

                            if "function_call" in message:
                                function_call = message["function_call"]
                                if function_call["name"] == "generate_documentation":
                                    arguments = function_call["arguments"]
                                    try:
                                        documentation = json.loads(arguments)
                                        logger.debug("Received documentation via function_call.")
                                        return documentation
                                    except json.JSONDecodeError as e:
                                        logger.error(f"Error decoding JSON from function_call arguments: {e}")
                                        logger.error(f"Arguments Content: {arguments}")
                                        continue
                            logger.error("No valid function_call found in the response.")
                        else:
                            logger.error("No choices found in the API response.")
                    else:
                        error_text = await response.text()
                        logger.error(f"Request failed with status {response.status}: {error_text}")

            if attempt < retry:
                wait_time = min(2 ** attempt, 16)
                logger.info(f"Retrying after {wait_time} seconds... (Attempt {attempt}/{retry})")
                await asyncio.sleep(wait_time)
        except Exception as e:
            logger.error(f"An unexpected error occurred during API request: {e}", exc_info=True)
            if attempt < retry:
                wait_time = min(2 ** attempt, 16)
                logger.info(f"Retrying after {wait_time} seconds... (Attempt {attempt}/{retry})")
                await asyncio.sleep(wait_time)

    logger.error("All retry attempts to fetch documentation failed.")
    return None

async def process_file(
    session: aiohttp.ClientSession,
    file_path: str,
    skip_types: Set[str],
    semaphore: asyncio.Semaphore,
    deployment_name: str,
    function_schema: Dict[str, Any],
    repo_root: str,
    project_info: str,
    style_guidelines: str,
    safe_mode: bool,
    azure_api_key: str,
    azure_endpoint: str,
    azure_api_version: str,
    output_dir: str
) -> Optional[str]:
    """
    Processes a single file to extract its structure and generate documentation.

    Args:
        session (aiohttp.ClientSession): The HTTP session for making requests.
        file_path (str): Path to the file to process.
        skip_types (Set[str]): Set of file extensions to skip.
        semaphore (asyncio.Semaphore): Semaphore to limit concurrent API requests.
        deployment_name (str): The Azure OpenAI deployment name.
        function_schema (Dict[str, Any]): The schema defining functions.
        repo_root (str): Root directory of the repository.
        project_info (str): Information about the project.
        style_guidelines (str): Documentation style guidelines.
        safe_mode (bool): If True, no files will be modified.
        azure_api_key (str): The API key for Azure OpenAI.
        azure_endpoint (str): The endpoint URL for the Azure OpenAI service.
        azure_api_version (str): The API version to use.
        output_dir (str): Directory to save documentation files.

    Returns:
        Optional[str]: The content of the documentation report or None if processing fails.
    """
    logger.debug(f'Processing file: {file_path}')
    try:
        _, ext = os.path.splitext(file_path)
        if not is_valid_extension(ext, skip_types) or is_binary(file_path):
            logger.debug(f"Skipping file '{file_path}' due to invalid extension or binary content.")
            return None

        language = get_language(ext)
        logger.debug(f"Detected language for '{file_path}': {language}")

        handler: Optional[BaseHandler] = get_handler(language, function_schema)
        if handler is None:
            logger.warning(f'Unsupported language: {language}')
            return None

        logger.info(f'Processing file: {file_path}')

        try:
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                content = await f.read()
            logger.debug(f"File content for '{file_path}' read successfully.")
        except Exception as e:
            logger.error(f"Failed to read '{file_path}': {e}", exc_info=True)
            return None

        documentation = None
        code_structure = None

        try:
            code_structure = await extract_code_structure(content, file_path, language, handler)
            if not code_structure:
                logger.warning(f"Could not extract code structure from '{file_path}'")
            else:
                logger.debug(f"Extracted code structure for '{file_path}': {code_structure}")
                prompt = generate_documentation_prompt(
                    file_name=os.path.basename(file_path),
                    code_structure=code_structure,
                    project_info=project_info,
                    style_guidelines=style_guidelines,
                    language=language,
                    function_schema=function_schema
                )
                documentation = await fetch_documentation_rest(
                    session=session,
                    prompt=prompt,
                    semaphore=semaphore,
                    deployment_name=deployment_name,
                    function_schema=function_schema,
                    azure_api_key=azure_api_key,
                    azure_endpoint=azure_endpoint,
                    azure_api_version=azure_api_version
                )
                if not documentation:
                    logger.error(f"Failed to generate documentation for '{file_path}'.")
                else:
                    # Combine code_structure with documentation as per schema
                    documentation['halstead'] = code_structure.get('halstead', {})
                    documentation['maintainability_index'] = code_structure.get('maintainability_index', None)
                    documentation['variables'] = code_structure.get('variables', [])
                    documentation['constants'] = code_structure.get('constants', [])
                    # Ensure 'changes_made' exists as per schema
                    documentation['changes_made'] = documentation.get('changes_made', [])
                    # Update functions and methods with complexity
                    function_complexity = {}
                    for func in code_structure.get('functions', []):
                        function_complexity[func['name']] = func.get('complexity', 0)
                    for func in documentation.get('functions', []):
                        func_name = func['name']
                        func['complexity'] = function_complexity.get(func_name, 0)
                    class_complexity = {}
                    for cls in code_structure.get('classes', []):
                        class_name = cls['name']
                        methods_complexity = {}
                        for method in cls.get('methods', []):
                            methods_complexity[method['name']] = method.get('complexity', 0)
                        class_complexity[class_name] = methods_complexity
                    for cls in documentation.get('classes', []):
                        class_name = cls['name']
                        methods_complexity = class_complexity.get(class_name, {})
                        for method in cls.get('methods', []):
                            method_name = method['name']
                            method['complexity'] = methods_complexity.get(method_name, 0)
        except Exception as e:
            logger.error(f"Error during code structure extraction or documentation generation for '{file_path}': {e}", exc_info=True)

        new_content = content

        if documentation and not safe_mode:
            try:
                new_content = await asyncio.to_thread(handler.insert_docstrings, content, documentation)

                if language.lower() == 'python':
                    new_content = await clean_unused_imports_async(new_content, file_path)
                    new_content = await format_with_black_async(new_content)

                is_valid = await asyncio.to_thread(handler.validate_code, new_content, file_path)
                if is_valid:
                    await backup_and_write_new_content(file_path, new_content)
                    logger.info(f"Documentation inserted into '{file_path}'")
                else:
                    logger.error(f"Code validation failed for '{file_path}'.")
            except Exception as e:
                logger.error(f"Error processing code documentation for '{file_path}': {e}", exc_info=True)
                new_content = content

        file_content = await write_documentation_report(
            documentation=documentation or {},
            language=language,
            file_path=file_path,
            repo_root=repo_root,
            output_dir=output_dir
        )
        logger.info(f"Finished processing '{file_path}'")
        return file_content

    except Exception as e:
        logger.error(f"Error processing file '{file_path}': {e}", exc_info=True)
        return None

async def process_all_files(
    session: aiohttp.ClientSession,
    file_paths: List[str],
    skip_types: Set[str],
    semaphore: asyncio.Semaphore,
    deployment_name: str,
    function_schema: Dict[str, Any],
    repo_root: str,
    project_info: Optional[str],
    style_guidelines: Optional[str],
    safe_mode: bool = False,
    output_file: str = 'output.md',
    azure_api_key: str = '',
    azure_endpoint: str = '',
    azure_api_version: str = '',
    output_dir: str = 'documentation'
) -> None:
    """
    Processes multiple files to extract their structures and generate documentation.

    Args:
        session (aiohttp.ClientSession): The HTTP session for making requests.
        file_paths (List[str]): List of file paths to process.
        skip_types (Set[str]): Set of file extensions to skip.
        semaphore (asyncio.Semaphore): Semaphore to limit concurrent API requests.
        deployment_name (str): The Azure OpenAI deployment name.
        function_schema (Dict[str, Any]): The schema defining functions.
        repo_root (str): Root directory of the repository.
        project_info (Optional[str]): Information about the project.
        style_guidelines (Optional[str]): Documentation style guidelines.
        safe_mode (bool, optional): If True, no files will be modified. Defaults to False.
        output_file (str, optional): Path to the output Markdown file. Defaults to 'output.md'.
        azure_api_key (str, optional): The API key for Azure OpenAI. Defaults to ''.
        azure_endpoint (str, optional): The endpoint URL for the Azure OpenAI service. Defaults to ''.
        azure_api_version (str, optional): The API version to use. Defaults to ''.
        output_dir (str, optional): Directory to save documentation files. Defaults to 'documentation'.
    """
    logger.info('Starting process of all files.')
    tasks = [
        process_file(
            session=session,
            file_path=file_path,
            skip_types=skip_types,
            semaphore=semaphore,
            deployment_name=deployment_name,
            function_schema=function_schema,
            repo_root=repo_root,
            project_info=project_info,
            style_guidelines=style_guidelines,
            safe_mode=safe_mode,
            azure_api_key=azure_api_key,
            azure_endpoint=azure_endpoint,
            azure_api_version=azure_api_version,
            output_dir=output_dir
        )
        for file_path in file_paths
    ]

    documentation_contents = []
    for f in asyncio.as_completed(tasks):
        try:
            file_content = await f
            if file_content:
                documentation_contents.append(file_content)
        except Exception as e:
            logger.error(f'Error processing a file: {e}', exc_info=True)
            if 'sentry_sdk' in globals():
                sentry_sdk.capture_exception(e)

    logger.info('Completed processing all files.')

    final_content = '\n\n'.join(documentation_contents)

    if final_content:
        toc = generate_table_of_contents(final_content)
        report_content = '# Documentation Generation Report\n\n## Table of Contents\n\n' + toc + '\n\n' + final_content

        try:
            async with aiofiles.open(output_file, 'w', encoding='utf-8') as f:
                await f.write(report_content)
            logger.info(f"Documentation report written to '{output_file}'")
        except Exception as e:
            logger.error(f"Error writing final documentation to '{output_file}': {e}", exc_info=True)
            if 'sentry_sdk' in globals():
                sentry_sdk.capture_exception(e)
    else:
        logger.warning("No documentation was generated.")

    logger.info('Running Flake8 on processed files for final linting.')
    for file_path in file_paths:
        _, ext = os.path.splitext(file_path)
        if ext.lower() in {'.py'}:
            flake8_output = await run_flake8_async(file_path)
            if flake8_output:
                logger.warning(f'Flake8 issues found in {file_path}:\n{flake8_output}')
    logger.info('Flake8 linting completed.')
```

## write_documentation_report.py

```python
import aiofiles
import re
import json
import os
import textwrap
import logging
import sys
from typing import Optional, Dict, Any, List
from utils import logger

if not logger.hasHandlers():
    handler = logging.StreamHandler(sys.stdout)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.DEBUG)

def get_threshold(metric: str, key: str, default: int) -> int:
    try:
        return int(os.getenv(f"{metric.upper()}_{key.upper()}_THRESHOLD", default))
    except ValueError:
        logger.error(f"Invalid environment variable for {metric.upper()}_{key.upper()}_THRESHOLD")
        return default

def format_table(headers: List[str], rows: List[List[str]]) -> str:
    table = "| " + " | ".join(headers) + " |\n"
    table += "| " + " | ".join(["---"] * len(headers)) + " |\n"
    for row in rows:
        table += "| " + " | ".join(row) + " |\n"
    return table

def generate_all_badges(
    complexity: Optional[int] = None,
    halstead: Optional[dict] = None,
    mi: Optional[float] = None
) -> str:
    badges = []

    if complexity is not None:
        low_threshold = get_threshold('complexity', 'low', 10)
        medium_threshold = get_threshold('complexity', 'medium', 20)
        color = "green" if complexity < low_threshold else "yellow" if complexity < medium_threshold else "red"
        complexity_badge = f'![Complexity](https://img.shields.io/badge/Complexity-{complexity}-{color}.svg?style=flat)'
        badges.append(complexity_badge)

    if halstead:
        volume = halstead.get('volume', 0)
        difficulty = halstead.get('difficulty', 0)
        effort = halstead.get('effort', 0)

        volume_low = get_threshold('halstead_volume', 'low', 100)
        volume_medium = get_threshold('halstead_volume', 'medium', 500)
        volume_color = "green" if volume < volume_low else "yellow" if volume < volume_medium else "red"

        difficulty_low = get_threshold('halstead_difficulty', 'low', 10)
        difficulty_medium = get_threshold('halstead_difficulty', 'medium', 20)
        difficulty_color = "green" if difficulty < difficulty_low else "yellow" if difficulty < difficulty_medium else "red"

        effort_low = get_threshold('halstead_effort', 'low', 500)
        effort_medium = get_threshold('halstead_effort', 'medium', 1000)
        effort_color = "green" if effort < effort_low else "yellow" if effort < effort_medium else "red"

        volume_badge = f'![Volume](https://img.shields.io/badge/Halstead%20Volume-{volume}-{volume_color}.svg?style=flat)'
        difficulty_badge = f'![Difficulty](https://img.shields.io/badge/Halstead%20Difficulty-{difficulty}-{difficulty_color}.svg?style=flat)'
        effort_badge = f'![Effort](https://img.shields.io/badge/Halstead%20Effort-{effort}-{effort_color}.svg?style=flat)'

        badges.extend([volume_badge, difficulty_badge, effort_badge])

    if mi is not None:
        high_threshold = get_threshold('maintainability_index', 'high', 80)
        medium_threshold = get_threshold('maintainability_index', 'medium', 50)
        color = "green" if mi > high_threshold else "yellow" if mi > medium_threshold else "red"
        mi_badge = f'![Maintainability](https://img.shields.io/badge/Maintainability-{mi:.2f}-{color}.svg?style=flat)'
        badges.append(mi_badge)

    return ' '.join(badges).strip()

def truncate_description(description: str, max_length: int = 100) -> str:
    return (description[:max_length] + '...') if len(description) > max_length else description

def sanitize_text(text: str) -> str:
    markdown_special_chars = ['*', '_', '`', '~', '<', '>', '#']
    for char in markdown_special_chars:
        text = text.replace(char, f"\\{char}")
    return text.replace('|', '\\|').replace('\n', ' ').strip()

def generate_table_of_contents(content: str) -> str:
    toc = []
    for line in content.splitlines():
        if line.startswith("#"):
            level = line.count("#")
            title = line.lstrip("#").strip()
            anchor = re.sub(r'[^a-zA-Z0-9\s-]', '', title)
            anchor = re.sub(r'\s+', '-', anchor).lower()
            anchor = re.sub(r'-+', '-', anchor).strip('-')
            toc.append(f"{'  ' * (level - 1)}- [{title}](#{anchor})")
    return "\n".join(toc)

def format_halstead_metrics(halstead: Dict[str, Any]) -> str:
    if not halstead:
        return ''
    volume = halstead.get('volume', 0)
    difficulty = halstead.get('difficulty', 0)
    effort = halstead.get('effort', 0)

    volume_low, volume_medium = 100, 500
    difficulty_low, difficulty_medium = 10, 20
    effort_low, effort_medium = 500, 1000

    volume_color = "green" if volume < volume_low else "yellow" if volume < volume_medium else "red"
    difficulty_color = "green" if difficulty < difficulty_low else "yellow" if difficulty < difficulty_medium else "red"
    effort_color = "green" if effort < effort_low else "yellow" if effort < effort_medium else "red"

    metrics = f'![Halstead Volume](https://img.shields.io/badge/Halstead%20Volume-{volume}-{volume_color}.svg?style=flat)\n'
    metrics += f'![Halstead Difficulty](https://img.shields.io/badge/Halstead%20Difficulty-{difficulty}-{difficulty_color}.svg?style=flat)\n'
    metrics += f'![Halstead Effort](https://img.shields.io/badge/Halstead%20Effort-{effort}-{effort_color}.svg?style=flat)\n'
    return metrics

def format_maintainability_index(mi_score: float) -> str:
    if mi_score is None:
        return ''
    return f'![Maintainability Index](https://img.shields.io/badge/Maintainability%20Index-{mi_score:.2f}-brightgreen.svg?style=flat)\n'

def format_methods(methods: List[Dict[str, Any]]) -> str:
    headers = ["Method Name", "Complexity", "Async", "Docstring"]
    rows = [
        [
            method.get("name", "N/A"),
            str(method.get("complexity", 0)),
            str(method.get("async", False)),
            sanitize_text(method.get("docstring", ""))
        ]
        for method in methods
    ]
    return format_table(headers, rows)

def format_classes(classes: List[Dict[str, Any]]) -> str:
    headers = ["Class Name", "Docstring"]
    rows = [
        [
            cls.get("name", "N/A"),
            sanitize_text(cls.get("docstring", ""))
        ]
        for cls in classes
    ]
    class_table = format_table(headers, rows)
    
    method_tables = []
    for cls in classes:
        if cls.get("methods"):
            method_tables.append(f"#### Methods for {cls.get('name')}\n")
            method_tables.append(format_methods(cls.get("methods", [])))
    
    return class_table + "\n\n" + "\n".join(method_tables)

def format_functions(functions: List[Dict[str, Any]]) -> str:
    headers = ["Function Name", "Complexity", "Async", "Docstring"]
    rows = [
        [
            func.get("name", "N/A"),
            str(func.get("complexity", 0)),
            str(func.get("async", False)),
            sanitize_text(func.get("docstring", ""))
        ]
        for func in functions
    ]
    return format_table(headers, rows)

def format_variables_and_constants(variables: List[Dict[str, Any]], constants: List[Dict[str, Any]]) -> str:
    headers = ["Name", "Type", "Data Type", "Description", "Defined At", "Usage Example", "References"]
    rows = []

    for var in variables:
        row = [
            var.get("name", "N/A"),
            "Variable",
            f"`{var.get('type', 'Unknown')}`",
            sanitize_text(var.get("description", "No description provided.")),
            f"[{var.get('file', 'N/A')}:{var.get('line', 'N/A')}]({var.get('link', '#')})",
            sanitize_text(var.get("example", "No example provided.")),
            sanitize_text(var.get("references", "N/A"))
        ]
        rows.append(row)

    for const in constants:
        row = [
            const.get("name", "N/A"),
            "Constant",
            f"`{const.get('type', 'Unknown')}`",
            sanitize_text(const.get("description", "No description provided.")),
            f"[{const.get('file', 'N/A')}:{const.get('line', 'N/A')}]({const.get('link', '#')})",
            sanitize_text(const.get("example", "No example provided.")),
            sanitize_text(const.get("references", "N/A"))
        ]
        rows.append(row)

    return format_table(headers, rows)

def generate_summary(variables: List[Dict[str, Any]], constants: List[Dict[str, Any]]) -> str:
    total_vars = len(variables)
    total_consts = len(constants)
    summary = f"### **Summary**\n\n- **Total Variables:** {total_vars}\n- **Total Constants:** {total_consts}\n"
    return summary

def sanitize_filename(filename: str) -> str:
    return re.sub(r'[<>:"/\\|?*]', '_', filename)

def generate_documentation_prompt(
    file_name: str,
    code_structure: Dict[str, Any],
    project_info: str,
    style_guidelines: str,
    language: str,
    function_schema: Dict[str, Any]
) -> str:
    functions = function_schema.get("functions", [])
    if functions and "parameters" in functions[0]:
        schema = json.dumps(function_schema["functions"][0]["parameters"], indent=2)
    else:
        logger.error("Function schema is missing or empty.")
        schema = "{}"  # Fallback or handle accordingly

    prompt = f"""
    You are a code documentation generator.
    
    Project Info:
    {project_info}
    
    Style Guidelines:
    {style_guidelines}
    
    Given the following code structure of the {language} file '{file_name}', generate detailed documentation according to the specified schema.
    
    Code Structure:
    {json.dumps(code_structure, indent=2)}
    
    Schema:
    {schema}
    
    Ensure that the output is a JSON object that follows the schema exactly, including all required fields.
    
    Example Output:
    {{
      "summary": "Brief summary of the file.",
      "changes_made": ["List of changes made to the file."],
      "functions": [
        {{
          "name": "function_name",
          "docstring": "Detailed description of the function.",
          "args": ["arg1", "arg2"],
          "async": false
        }}
      ],
      "classes": [
        {{
          "name": "ClassName",
          "docstring": "Detailed description of the class.",
          "methods": [
            {{
              "name": "method_name",
              "docstring": "Detailed description of the method.",
              "args": ["arg1"],
              "async": false,
              "type": "instance"
            }}
          ]
        }}
      ]
    }}
    
    Ensure all strings are properly escaped and the JSON is valid.
    
    Output:"""
    return textwrap.dedent(prompt).strip()

async def write_documentation_report(
    documentation: Optional[dict],
    language: str,
    file_path: str,
    repo_root: str,
    output_dir: str
) -> str:
    try:
        if not documentation:
            logger.warning(f"No documentation to write for '{file_path}'")
            return ''

        relative_path = os.path.relpath(file_path, repo_root)
        safe_file_name = sanitize_filename(relative_path.replace(os.sep, '_'))
        doc_file_path = os.path.join(output_dir, f"{safe_file_name}.md")

        file_header = f'# File: {relative_path}\n\n'
        documentation_content = file_header

        # Add Badges
        badges = generate_all_badges(
            complexity=documentation.get('complexity'),
            halstead=documentation.get('halstead', {}),
            mi=documentation.get('maintainability_index')
        )
        documentation_content += badges + "\n\n"

        # Add Halstead metrics and Maintainability Index
        halstead_content = format_halstead_metrics(documentation.get('halstead', {}))
        mi_content = format_maintainability_index(documentation.get('maintainability_index'))
        documentation_content += halstead_content + mi_content + "\n"

        # Add Summary
        summary = generate_summary(documentation.get('variables', []), documentation.get('constants', []))
        documentation_content += summary + "\n"

        # Add Changes Made
        changes_made = documentation.get('changes_made', [])
        if changes_made:
            documentation_content += f"## Changes Made\n\n"
            for change in changes_made:
                documentation_content += f"- {sanitize_text(change)}\n"
            documentation_content += "\n"

        # Add Classes and Methods
        classes = documentation.get('classes', [])
        if classes:
            documentation_content += "## Classes\n\n"
            documentation_content += format_classes(classes)
            documentation_content += "\n"

        # Add Functions
        functions = documentation.get('functions', [])
        if functions:
            documentation_content += "## Functions\n\n"
            documentation_content += format_functions(functions)
            documentation_content += "\n"

        # Add Variables and Constants
        variables = documentation.get('variables', [])
        constants = documentation.get('constants', [])
        if variables or constants:
            documentation_content += "## Variables and Constants\n\n"
            documentation_content += format_variables_and_constants(variables, constants)
            documentation_content += "\n"

        # Add Source Code
        async with aiofiles.open(file_path, 'r', encoding='utf-8') as file:
            source_code = await file.read()
        documentation_content += f"## Source Code\n\n```{language}\n{source_code}\n```\n"

        # Generate Table of Contents
        toc = generate_table_of_contents(documentation_content)
        documentation_content = "# Table of Contents\n\n" + toc + "\n\n" + documentation_content

        # Create the output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)

        # Write to markdown file
        async with aiofiles.open(doc_file_path, 'w', encoding='utf-8') as f:
            await f.write(documentation_content)
        logger.info(f"Documentation written to '{doc_file_path}' successfully.")
        return documentation_content

    except FileNotFoundError as e:
        logger.error(f"File not found: {e}")
        return ''
    except subprocess.SubprocessError as e:
        logger.error(f"Subprocess error during flake8 execution: {e}")
        return ''
    except Exception as e:
        logger.error(f"Unexpected error: {e} for file {file_path}", exc_info=True)
        return ''
```

## .flake8

```
[flake8]
max-line-length = 120

```

## utils.py

```python
# utils.py

import os
import sys
import json
import logging
import aiohttp
import asyncio
import subprocess
from dotenv import load_dotenv
from typing import Any, Set, List, Optional, Dict, Tuple
from jsonschema import validate, ValidationError

# Load environment variables
load_dotenv()

AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
API_VERSION = os.getenv("API_VERSION")

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    handlers=[
        logging.FileHandler("documentation_generation.log"),
        logging.StreamHandler(sys.stdout)
    ],
)
logger = logging.getLogger(__name__)

# ----------------------------
# Constants
# ----------------------------

DEFAULT_EXCLUDED_DIRS = {'.git', '__pycache__', 'node_modules', '.venv', '.idea', 'scripts'}
DEFAULT_EXCLUDED_FILES = {".DS_Store"}
DEFAULT_SKIP_TYPES = {".json", ".md", ".txt", ".csv", ".lock"}

LANGUAGE_MAPPING = {
    ".py": "python",
    ".js": "javascript",
    ".jsx": "javascript",
    ".ts": "typescript",
    ".tsx": "typescript",
    ".html": "html",
    ".htm": "html",
    ".css": "css",
    ".go": "go",
    ".cpp": "cpp",
    ".c": "cpp",
    ".java": "java",
}

# ----------------------------
# Language and File Utilities
# ----------------------------

def get_language(ext: str) -> str:
    """
    Determines the programming language based on file extension.

    Args:
        ext (str): File extension.

    Returns:
        str: Corresponding programming language.
    """
    language = LANGUAGE_MAPPING.get(ext.lower(), "plaintext")
    logger.debug(f"Detected language for extension '{ext}': {language}")
    return language

def is_valid_extension(ext: str, skip_types: Set[str]) -> bool:
    """
    Checks if a file extension is valid (not in the skip list).

    Args:
        ext (str): File extension.
        skip_types (Set[str]): Set of file extensions to skip.

    Returns:
        bool: True if valid, False otherwise.
    """
    is_valid = ext.lower() not in skip_types
    logger.debug(f"Extension '{ext}' is valid: {is_valid}")
    return is_valid

def is_binary(file_path: str) -> bool:
    """
    Checks if a file is binary.

    Args:
        file_path (str): Path to the file.

    Returns:
        bool: True if binary, False otherwise.
    """
    try:
        with open(file_path, "rb") as file:
            return b"\0" in file.read(1024)
    except Exception as e:
        logger.error(f"Error checking if file is binary '{file_path}': {e}")
        return True

def get_all_file_paths(repo_path: str, excluded_dirs: Set[str], excluded_files: Set[str], skip_types: Set[str]) -> List[str]:
    """
    Retrieves all file paths in the repository, excluding specified directories and files.

    Args:
        repo_path (str): Path to the repository.
        excluded_dirs (Set[str]): Set of directories to exclude.
        excluded_files (Set[str]): Set of files to exclude.
        skip_types (Set[str]): Set of file extensions to skip.

    Returns:
        List[str]: List of file paths.
    """
    file_paths = []
    normalized_excluded_dirs = {os.path.normpath(os.path.join(repo_path, d)) for d in excluded_dirs}

    for root, dirs, files in os.walk(repo_path, topdown=True):
        # Exclude directories
        dirs[:] = [d for d in dirs if os.path.normpath(os.path.join(root, d)) not in normalized_excluded_dirs]

        for file in files:
            # Exclude files
            if file in excluded_files:
                continue
            file_ext = os.path.splitext(file)[1]
            # Skip specified file types
            if file_ext in skip_types:
                continue
            full_path = os.path.join(root, file)
            file_paths.append(full_path)
    logger.debug(f"Collected {len(file_paths)} files from '{repo_path}'.")
    return file_paths

# ----------------------------
# Configuration Management
# ----------------------------

def load_json_schema(schema_path: str) -> Optional[dict]:
    """
    Loads a JSON schema from the specified path.

    Args:
        schema_path (str): Path to the JSON schema file.

    Returns:
        Optional[dict]: Loaded JSON schema or None if failed.
    """
    try:
        with open(schema_path, "r", encoding="utf-8") as f:
            schema = json.load(f)
        logger.debug(f"Successfully loaded JSON schema from '{schema_path}'.")
        return schema
    except FileNotFoundError:
        logger.error(f"JSON schema file '{schema_path}' not found.")
        return None
    except json.JSONDecodeError as e:
        logger.error(f"Error decoding JSON from '{schema_path}': {e}")
        return None
    except Exception as e:
        logger.error(f"Unexpected error loading JSON schema from '{schema_path}': {e}")
        return None

def load_function_schema(schema_path: str) -> dict:
    """Loads a function schema from a specified path.

    Args:
        schema_path (str): Path to the schema file.

    Returns:
        dict: The function schema loaded from the file."""
    logger.debug(f"Attempting to load function schema from '{schema_path}'.")
    schema = load_json_schema(schema_path)
    if not isinstance(schema, dict):
        logger.critical(f"Function schema should be a JSON object with a 'functions' key. Found type: {type(schema)}")
        sys.exit(1)
    if 'functions' not in schema:
        logger.critical(f"Function schema missing 'functions' key.")
        sys.exit(1)
    return schema

def load_config(config_path: str, excluded_dirs: Set[str], excluded_files: Set[str], skip_types: Set[str]) -> Tuple[str, str]:
    """
    Loads additional configurations from a config.json file.

    Args:
        config_path (str): Path to the config.json file.
        excluded_dirs (Set[str]): Set to update with excluded directories.
        excluded_files (Set[str]): Set to update with excluded files.
        skip_types (Set[str]): Set to update with file types to skip.

    Returns:
        Tuple[str, str]: Project information and style guidelines.
    """
    try:
        with open(config_path, "r", encoding="utf-8") as f:
            config = json.load(f)
        project_info = config.get("project_info", "")
        style_guidelines = config.get("style_guidelines", "")
        excluded_dirs.update(config.get("excluded_dirs", []))
        excluded_files.update(config.get("excluded_files", []))
        skip_types.update(config.get("skip_types", []))
        logger.debug(f"Loaded configuration from '{config_path}'.")
        return project_info, style_guidelines
    except FileNotFoundError:
        logger.error(f"Config file '{config_path}' not found.")
        return "", ""
    except json.JSONDecodeError as e:
        logger.error(f"Error decoding JSON from '{config_path}': {e}")
        return "", ""
    except Exception as e:
        logger.error(f"Unexpected error loading config file '{config_path}': {e}")
        return "", ""

# ----------------------------
# Code Formatting and Cleanup
# ----------------------------

def format_table(headers: list, rows: list) -> str:
    table = "| " + " | ".join(headers) + " |\n"
    table += "| " + " | ".join(["---"] * len(headers)) + " |\n"
    for row in rows:
        table += "| " + " | ".join(row) + " |\n"
    return table

def truncate_description(description: str, max_length: int = 100) -> str:
    return (description[:max_length] + '...') if len(description) > max_length else description


async def clean_unused_imports_async(code: str, file_path: str) -> str:
    """
    Asynchronously removes unused imports and variables from the provided code using autoflake.

    Args:
        code (str): The source code to clean.
        file_path (str): The file path used for display purposes in autoflake.

    Returns:
        str: The cleaned code with unused imports and variables removed.
    """
    try:
        process = await asyncio.create_subprocess_exec(
            'autoflake', '--remove-all-unused-imports', '--remove-unused-variables', '--stdin-display-name', file_path, '-',
            stdin=asyncio.subprocess.PIPE,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await process.communicate(input=code.encode())
        
        if process.returncode != 0:
            logger.error(f"Autoflake failed:\n{stderr.decode()}")
            return code
        
        return stdout.decode()
    except Exception as e:
        logger.error(f'Error running Autoflake: {e}')
        return code

# Example using asyncio subprocesses
async def format_with_black_async(code: str) -> str:
    process = await asyncio.create_subprocess_exec(
        'black', '--quiet', '-',
        stdin=asyncio.subprocess.PIPE,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE
    )
    stdout, stderr = await process.communicate(input=code.encode())
    if process.returncode == 0:
        return stdout.decode()
    else:
        logger.error(f"Black formatting failed: {stderr.decode()}")
        return code


async def run_flake8_async(file_path: str) -> Optional[str]:
    """
    Asynchronously runs Flake8 on the specified file to check for style violations.

    Args:
        file_path (str): The path to the file to be checked.

    Returns:
        Optional[str]: The output from Flake8 if there are violations, otherwise None.
    """
    try:
        process = await asyncio.create_subprocess_exec(
            'flake8', file_path,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await process.communicate()
        
        if process.returncode != 0:
            return stdout.decode() + stderr.decode()
        
        return None
    except Exception as e:
        logger.error(f'Error running Flake8: {e}')
        return None

# ----------------------------
# JavaScript/TypeScript Utilities
# ----------------------------

async def run_node_script_async(script_path: str, input_json: str) -> Optional[str]:
    try:
        process = await asyncio.create_subprocess_exec(
            'node', script_path,
            stdin=asyncio.subprocess.PIPE,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await process.communicate(input=input_json.encode())
        if process.returncode != 0:
            logger.error(f"Node script error:\n{stderr.decode()}")
            return None
        return stdout.decode()
    except Exception as e:
        logger.error(f'Error running Node script: {e}')
        return None

def run_node_insert_docstrings(script_name: str, input_data: dict) -> Optional[str]:
    """
    Runs a Node.js script to insert docstrings and returns the modified code.

    Args:
        script_name (str): Name of the script to run.
        input_data (dict): Input data to pass to the script.

    Returns:
        Optional[str]: The modified code if successful, None otherwise.
    """
    try:
        script_path = os.path.join(os.path.dirname(__file__), 'scripts', script_name)
        logger.debug(f"Running Node.js script: {script_path}")

        input_json = json.dumps(input_data)
        result = subprocess.run(
            ["node", script_path],
            input=input_json,
            capture_output=True,
            text=True,
            check=True
        )
        logger.debug(f"Successfully ran {script_path}")
        return result.stdout
    except subprocess.CalledProcessError as e:
        logger.error(f"Error running {script_name}: {e.stderr}")
        return None
    except FileNotFoundError:
        logger.error(f"Node.js script {script_name} not found.")
        return None
    except Exception as e:
        logger.error(f"Unexpected error running {script_name}: {e}")
        return None

# ----------------------------
# Documentation Generation
# ----------------------------
# ----------------------------
# Schema Validation
# ----------------------------

def validate_schema(schema: dict):
    """
    Validates the loaded schema against a predefined schema.

    Args:
        schema (dict): The schema to validate.
    """
    predefined_schema = {
        "type": "object",
        "properties": {
            "summary": {"type": "string"},
            "changes_made": {"type": "array", "items": {"type": "string"}},
            "functions": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "name": {"type": "string"},
                        "args": {"type": "array", "items": {"type": "string"}},
                        "docstring": {"type": "string"},
                        "async": {"type": "boolean"},
                        "complexity": {"type": "integer"}
                    },
                    "required": ["name", "args", "docstring", "async", "complexity"]
                }
            },
            "classes": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "name": {"type": "string"},
                        "docstring": {"type": "string"},
                        "methods": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "name": {"type": "string"},
                                    "args": {"type": "array", "items": {"type": "string"}},
                                    "docstring": {"type": "string"},
                                    "async": {"type": "boolean"},
                                    "type": {"type": "string"},
                                    "complexity": {"type": "integer"}
                                },
                                "required": ["name", "args", "docstring", "async", "type", "complexity"]
                            }
                        }
                    },
                    "required": ["name", "docstring", "methods"]
                }
            }
        },
        "required": ["summary", "changes_made", "functions", "classes"]
    }
    try:
        validate(instance=schema, schema=predefined_schema)
        logger.debug("Documentation schema is valid.")
    except ValidationError as ve:
        logger.critical(f"Schema validation error: {ve.message}")
        sys.exit(1)

# ----------------------------
# EOF
# ----------------------------

```

## main.py

```python
import os
import sys
import logging
import argparse
import asyncio
import tracemalloc
import aiohttp
from dotenv import load_dotenv
from file_handlers import process_all_files
from utils import (
    load_config,
    get_all_file_paths,
    DEFAULT_EXCLUDED_DIRS,
    DEFAULT_EXCLUDED_FILES,
    DEFAULT_SKIP_TYPES,
    load_function_schema,
)

# Load environment variables from .env file early
load_dotenv()

# Enable tracemalloc
tracemalloc.start()

# Import Sentry SDK and integrations
import sentry_sdk
from sentry_sdk.integrations.logging import LoggingIntegration
from sentry_sdk.integrations.aiohttp import AioHttpIntegration

# Define the before_send function for data scrubbing and filtering
def before_send(event, hint):
    """
    Modify or filter out events before they are sent to Sentry.
    """
    # Scrub sensitive information
    if 'password' in event.get('request', {}).get('data', {}):
        event['request']['data']['password'] = '***REDACTED***'

    # Example: Filter out specific exceptions
    if event.get('exception'):
        exception_type = event['exception']['values'][0]['type']
        if exception_type in ['SomeNonCriticalException', 'IgnoredException']:
            return None  # Drop the event

    return event

# Configure Sentry Logging Integration
logging_integration = LoggingIntegration(
    level=logging.INFO,        # Capture info and above as breadcrumbs
    event_level=logging.ERROR  # Send errors as events
)

# Initialize Sentry
sentry_sdk.init(
    dsn=os.getenv("SENTRY_DSN"),
    integrations=[
        logging_integration,
        AioHttpIntegration(),  # Integrate with aiohttp for tracing
    ],
    traces_sample_rate=0.2,        # 20% sample rate for production
    environment=os.getenv("ENVIRONMENT", "production"),
    release=os.getenv("RELEASE_VERSION", "unknown"),
    before_send=before_send,       # Add the before_send callback
)

# Configure logging
logger = logging.getLogger(__name__)

def parse_arguments():
    parser = argparse.ArgumentParser(
        description="Generate and insert comments/docstrings using Azure OpenAI's REST API."
    )
    parser.add_argument("repo_path", help="Path to the code repository")
    parser.add_argument("-c", "--config", help="Path to config.json", default="config.json")
    parser.add_argument("--concurrency", help="Number of concurrent requests", type=int, default=5)
    parser.add_argument("-o", "--output", help="Output Markdown file", default="output.md")
    parser.add_argument("--deployment-name", help="Deployment name for Azure OpenAI", required=True)
    parser.add_argument("--skip-types", help="Comma-separated list of file extensions to skip", default="")
    parser.add_argument("--project-info", help="Information about the project", default="")
    parser.add_argument("--style-guidelines", help="Documentation style guidelines to follow", default="")
    parser.add_argument("--safe-mode", help="Run in safe mode (no files will be modified)", action="store_true")
    parser.add_argument("--log-level", help="Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)", default="INFO")
    parser.add_argument("--schema", help="Path to function_schema.json", default=os.path.join(
        os.path.dirname(os.path.abspath(__file__)), "schemas", "function_schema.json"))
    parser.add_argument("--doc-output-dir", help="Directory to save documentation files", default="documentation")
    return parser.parse_args()

def configure_logging(log_level):
    """
    Configures logging for the application.
    """
    numeric_level = getattr(logging, log_level.upper(), logging.INFO)
    logger.setLevel(numeric_level)
    formatter = logging.Formatter(
        "%(asctime)s [%(levelname)s] %(name)s:%(funcName)s:%(lineno)d: %(message)s"
    )

    # File handler for detailed logs
    file_handler = logging.FileHandler("docs_generation.log")
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)

    # Console handler for real-time feedback
    console_handler = logging.StreamHandler()
    console_handler.setLevel(numeric_level)
    console_handler.setFormatter(formatter)

    # Prevent adding multiple handlers if configure_logging is called multiple times
    if not logger.handlers:
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)

async def main():
    args = parse_arguments()

    configure_logging(args.log_level)

    logger.info("Starting Documentation Generation Tool.")
    logger.debug(f"Parsed arguments: {args}")

    repo_path = args.repo_path
    config_path = args.config
    concurrency = args.concurrency
    output_file = args.output
    deployment_name = args.deployment_name
    skip_types = args.skip_types
    project_info_arg = args.project_info
    style_guidelines_arg = args.style_guidelines
    safe_mode = args.safe_mode
    schema_path = args.schema
    output_dir = args.doc_output_dir  # Get the documentation output directory

    # Ensure necessary environment variables are set for Azure OpenAI Service
    AZURE_OPENAI_API_KEY = os.getenv('AZURE_OPENAI_API_KEY')
    AZURE_OPENAI_ENDPOINT = os.getenv('AZURE_OPENAI_ENDPOINT')
    AZURE_OPENAI_API_VERSION = os.getenv('API_VERSION')

    if not all([AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_VERSION, deployment_name]):
        logger.critical(
            "AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, API_VERSION, or DEPLOYMENT_NAME not set. "
            "Please set them in your environment or .env file."
        )
        sys.exit(1)
    logger.info("Using Azure OpenAI with Deployment ID: %s", deployment_name)

    logger.info(f"Repository Path: {repo_path}")
    logger.info(f"Configuration File: {config_path}")
    logger.info(f"Concurrency Level: {concurrency}")
    logger.info(f"Output Markdown File: {output_file}")
    logger.info(f"Deployment Name: {deployment_name}")
    logger.info(f"Safe Mode: {'Enabled' if safe_mode else 'Disabled'}")
    logger.info(f"Function Schema Path: {schema_path}")
    logger.info(f"Documentation Output Directory: {output_dir}")

    if not os.path.isdir(repo_path):
        logger.critical(f"Invalid repository path: '{repo_path}' is not a directory.")
        sys.exit(1)
    else:
        logger.debug(f"Repository path '{repo_path}' is valid.")

    excluded_dirs = set(DEFAULT_EXCLUDED_DIRS)
    excluded_files = set(DEFAULT_EXCLUDED_FILES)
    skip_types_set = set(DEFAULT_SKIP_TYPES)
    if skip_types:
        skip_types_set.update(
            ext.strip() if ext.strip().startswith(".") else f".{ext.strip()}"
            for ext in skip_types.split(",") if ext.strip()
        )
        logger.debug(f"Updated skip_types: {skip_types_set}")

    project_info_config = ""
    style_guidelines_config = ""

    if not os.path.isfile(config_path):
        logger.warning(
            f"Configuration file '{config_path}' not found. "
            "Proceeding with default and command-line settings."
        )
    else:
        project_info_config, style_guidelines_config = load_config(config_path, excluded_dirs, excluded_files, skip_types_set)

    project_info = project_info_arg or project_info_config
    style_guidelines = style_guidelines_arg or style_guidelines_config

    if project_info:
        logger.info(f"Project Info: {project_info}")
    if style_guidelines:
        logger.info(f"Style Guidelines: {style_guidelines}")

    # Load function schema
    function_schema = load_function_schema(schema_path)

    try:
        file_paths = get_all_file_paths(repo_path, excluded_dirs, excluded_files, skip_types_set)
    except Exception as e:
        logger.critical(f"Error retrieving file paths: {e}")
        sys.exit(1)

    # Start a Sentry transaction for the main documentation generation process
    with sentry_sdk.start_transaction(op="task", name="Documentation Generation"):
        async with aiohttp.ClientSession(raise_for_status=True) as session:
            await process_all_files(
                session=session,
                file_paths=file_paths,
                skip_types=skip_types_set,
                semaphore=asyncio.Semaphore(concurrency),
                deployment_name=deployment_name,
                function_schema=function_schema,
                repo_root=repo_path,
                project_info=project_info,
                style_guidelines=style_guidelines,
                safe_mode=safe_mode,
                output_file=output_file,
                azure_api_key=AZURE_OPENAI_API_KEY,
                azure_endpoint=AZURE_OPENAI_ENDPOINT,
                azure_api_version=AZURE_OPENAI_API_VERSION,
                output_dir=output_dir  # Pass the documentation output directory
            )

    logger.info("Documentation generation completed successfully.")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Process interrupted by user.")
    except Exception as e:
        logger.critical(f"Unhandled exception: {e}", exc_info=True)
        sys.exit(1)

```

## config.json

```
{
  "directory": "/root/docs",
  "excluded_dirs": [".git", ".github", ".vscode", "modules", "vids", "node_modules", "__pycache__", "pipmodules", "pip"],
  "excluded_files": [".env", "files.md", "documentation_generation.log", "docs_generation.log", "file_handlers.log", "language_functions.log", ".gitignore", "package.json", "package-lock.json", "README.md", "output.md", "docs_generation.log", "tsconfig.json"],
  "skip_types": [".bak"]
}
```
## language_functions/html_handler.py

```python
# language_functions/html_handler.py

import logging
import subprocess
from typing import Optional, Dict, Any
from bs4 import BeautifulSoup, Comment
from language_functions.base_handler import BaseHandler

logger = logging.getLogger(__name__)

class HTMLHandler(BaseHandler):
    """Handler for HTML language."""
    def __init__(self, function_schema: Dict[str, Any]):
        """Initializes the HTMLHandler with a function schema."""
        self.function_schema = function_schema
        
    def extract_structure(self, code: str) -> Dict[str, Any]:
        """Extracts the structure of HTML code."""
        logger.debug("Extracting HTML structure.")
        try:
            soup = BeautifulSoup(code, "lxml")
            structure = {"tags": []}
            for tag in soup.find_all(True):
                structure["tags"].append({"name": tag.name, "attributes": tag.attrs})
                logger.debug(f"Extracted tag: {tag.name}")
            return structure
        except Exception as e:
            logger.error(f"Error extracting HTML structure: {e}")
            return {}

    def insert_docstrings(self, code: str, documentation: Dict[str, Any]) -> str:
        """Inserts comments into HTML code based on provided documentation."""
        logger.debug("Inserting HTML comments.")
        try:
            soup = BeautifulSoup(code, "lxml")
            summary = documentation.get("summary", "").strip()
            changes = documentation.get("changes_made", [])
            if not summary and not changes:
                logger.warning("No summary or changes provided in documentation. Skipping comment insertion.")
                return code
            new_comment_parts = []
            if summary:
                new_comment_parts.append(f"Summary: {summary}")
            if changes:
                changes_formatted = "; ".join(changes)
                new_comment_parts.append(f"Changes: {changes_formatted}")
            new_comment = Comment(" " + " | ".join(new_comment_parts) + " ")
            if soup.body:
                soup.body.insert(0, new_comment)
                logger.debug("Inserted comment at the beginning of the body.")
            else:
                soup.insert(0, new_comment)
                logger.debug("Inserted comment at the beginning of the document.")
            modified_code = soup.prettify()
            logger.debug("Completed inserting HTML comments.")
            return modified_code
        except Exception as e:
            logger.error(f"Error inserting HTML comments: {e}")
            return code

    def validate_code(self, code: str, file_path: Optional[str] = None) -> bool:
        """
        Validates HTML code using the tidy utility.

        Args:
            code (str): The HTML code to validate.
            file_path (Optional[str]): The path to the HTML file being validated.

        Returns:
            bool: True if the code is valid, False otherwise.
        """
        logger.debug('Starting HTML code validation.')
        if not file_path:
            logger.warning('File path not provided for HTML validation. Skipping tidy validation.')
            return True  # Assuming no validation without a file

        try:
            # Write code to the specified file path
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(code)

            # Attempt to validate the HTML file using tidy
            process = subprocess.run(
                ['tidy', '-e', file_path],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )

            if process.returncode != 0:
                logger.error(f'Tidy validation failed for {file_path}:\n{process.stderr}')
                return False
            else:
                logger.debug('Tidy validation successful.')
            return True
        except FileNotFoundError:
            logger.error("Tidy utility not found. Please install it using your package manager (e.g., 'sudo apt-get install tidy').")
            return False
        except Exception as e:
            logger.error(f'Unexpected error during HTML code validation: {e}')
            return False
```

## language_functions/language_functions.py

```python
# language_functions.py

import json
import logging
from typing import Dict, Any
from .base_handler import PythonHandler  # Ensure correct import path

logger = logging.getLogger(__name__)

def get_handler(language: str, function_schema: Dict[str, Any]):
    """
    Returns the appropriate handler based on the programming language.

    Args:
        language (str): The programming language of the source code.
        function_schema (Dict[str, Any]): The function schema loaded from JSON.

    Returns:
        BaseHandler or None: An instance of the appropriate handler or None if unsupported.
    """
    language = language.lower()
    if language == 'python':
        return PythonHandler(function_schema)
    # Add other language handlers here (e.g., JavaHandler) as needed
    else:
        logger.warning(f"No handler available for language: {language}")
        return None

def insert_docstrings(
    original_code: str, documentation: Dict[str, Any], language: str, schema_path: str = 'schemas/function_schema.json'
) -> str:
    """
    Inserts docstrings/comments into code based on the language.

    Args:
        original_code (str): The original source code.
        documentation (Dict[str, Any]): Documentation details obtained from AI.
        language (str): Programming language of the source code.
        schema_path (str): Path to the function schema JSON file.

    Returns:
        str: The source code with inserted documentation.
    """
    logger.debug(f"Processing docstrings for language: {language}")

    # Load the function schema from the specified path
    function_schema = None
    try:
        with open(schema_path, 'r', encoding='utf-8') as f:
            function_schema = json.load(f)
        logger.debug(f"Loaded function schema from '{schema_path}'.")
    except FileNotFoundError:
        logger.error(f"Function schema file not found at '{schema_path}'.")
        return original_code
    except json.JSONDecodeError as e:
        logger.error(f"Error decoding JSON from function schema file: {e}")
        return original_code
    except OSError as e:
        logger.error(f"OS error occurred while loading function schema: {e}")
        return original_code
    except Exception as e:
        logger.error(f"Unexpected error loading function schema: {e}", exc_info=True)
        return original_code

    handler = get_handler(language, function_schema)
    if not handler:
        logger.warning(f"Unsupported language '{language}'. Skipping docstring insertion.")
        return original_code

    if documentation is None:
        logger.error("Documentation is None. Skipping docstring insertion.")
        return original_code

    # Insert docstrings using the handler
    try:
        updated_code = handler.insert_docstrings(original_code, documentation)
        logger.debug("Docstring insertion completed successfully.")
        return updated_code
    except Exception as e:
        logger.error(f"Error inserting docstrings: {e}", exc_info=True)
        return original_code

```

## language_functions/base_handler.py

```python
# language_functions/base_handler.py

import abc
import logging
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)

class BaseHandler(abc.ABC):
    """Abstract base class for language-specific handlers."""

    @abc.abstractmethod
    def extract_structure(self, code: str, file_path: str) -> Dict[str, Any]:
        """
        Extracts the structure of the code (classes, functions, etc.).

        Args:
            code (str): The source code to analyze.
            file_path (str): Path to the source file.

        Returns:
            Dict[str, Any]: A dictionary representing the code structure.
        """
        pass

    @abc.abstractmethod
    def insert_docstrings(self, code: str, documentation: Dict[str, Any]) -> str:
        """
        Inserts docstrings/comments into the code based on the documentation.

        Args:
            code (str): The original source code.
            documentation (Dict[str, Any]): Documentation details obtained from AI.

        Returns:
            str: The source code with inserted documentation.
        """
        pass

    @abc.abstractmethod
    def validate_code(self, code: str, file_path: Optional[str] = None) -> bool:
        """
        Validates the modified code for syntax correctness.

        Args:
            code (str): The modified source code.
            file_path (Optional[str]): Path to the source file (optional).

        Returns:
            bool: True if the code is valid, False otherwise.
        """
        pass

```

## language_functions/python_handler.py

```python
import logging
import os
import sys
import tempfile
import subprocess
import ast
from typing import Dict, Any, Optional, List

# External dependencies
try:
    from radon.complexity import cc_visit
    from radon.metrics import h_visit, mi_visit
except ImportError as e:
    logging.error("radon is not installed. Please install it using 'pip install radon'.")
    raise

try:
    import libcst as cst
    from libcst import FunctionDef, ClassDef, SimpleStatementLine, Expr, SimpleString
except ImportError as e:
    logging.error("libcst is not installed. Please install it using 'pip install libcst'.")
    raise

from language_functions.base_handler import BaseHandler

logger = logging.getLogger(__name__)

class PythonHandler(BaseHandler):
    """Handler for Python language."""

    def __init__(self, function_schema: Dict[str, Any]):
        """
        Initializes the PythonHandler with the provided function schema.

        Args:
            function_schema (Dict[str, Any]): The schema used for function operations.
        """
        self.function_schema = function_schema

    def extract_structure(self, code: str, file_path: str) -> Dict[str, Any]:
        """Extracts the structure of the Python code, analyzing functions, classes, and assignments.

        Args:
            code (str): The source code to analyze.
            file_path (str): The file path for code reference.

        Returns:
            Dict[str, Any]: A detailed structure of the code components.
        """
        try:
            tree = ast.parse(code)
            code_structure = {
                "classes": [],
                "functions": [],
                "variables": [],
                "constants": [],
                "halstead": {},
                "maintainability_index": None,
                "decorators": [],
                "context_managers": [],
                "comprehensions": [],
            }
            complexity_scores = cc_visit(code)
            function_complexity = {score.fullname: score.complexity for score in complexity_scores}
            halstead_metrics = h_visit(code)
            if halstead_metrics:
                metrics = halstead_metrics[0]
                code_structure["halstead"] = {
                    "volume": metrics.volume,
                    "difficulty": metrics.difficulty,
                    "effort": metrics.effort,
                }
            mi_score = mi_visit(code, True)
            code_structure["maintainability_index"] = mi_score

            class CodeVisitor(ast.NodeVisitor):
                """AST visitor for traversing Python code structures and extracting functional and class definitions."""

                def __init__(self, file_path: str):
                    """Initializes the CodeVisitor for traversing AST nodes."""
                    self.scope_stack = []
                    self.file_path = file_path
                    # To track parent nodes for comment extraction
                    self.comments = self._extract_comments(code, tree)

                def _extract_comments(self, code: str, tree: ast.AST) -> Dict[int, List[str]]:
                    """Extracts comments from the source code and maps them to line numbers.

                    Args:
                        code (str): The source code.
                        tree (ast.AST): The parsed AST.

                    Returns:
                        Dict[int, List[str]]: Mapping from line numbers to list of comments.
                    """
                    comments = {}
                    for lineno, line in enumerate(code.splitlines(), start=1):
                        stripped = line.strip()
                        if stripped.startswith("#"):
                            comment = stripped.lstrip("#").strip()
                            comments.setdefault(lineno, []).append(comment)
                    return comments

                def visit_FunctionDef(self, node: ast.FunctionDef):
                    """Visits a function definition node."""
                    self._visit_function(node)

                def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef):
                    """Visits an async function definition node."""
                    self._visit_function(node, is_async=True)

                def _visit_function(self, node: ast.FunctionDef, is_async: bool = False) -> None:
                    """Handles both sync and async functions."""
                    self.scope_stack.append(node)
                    full_name = ".".join([scope.name for scope in self.scope_stack if hasattr(scope, 'name')])
                    complexity = function_complexity.get(full_name, 0)
                    decorators = [ast.unparse(d) for d in node.decorator_list] if hasattr(ast, 'unparse') else []
                    docstring = ast.get_docstring(node) or ""
                    function_info = {
                        "name": node.name,
                        "docstring": docstring,
                        "args": [arg.arg for arg in node.args.args if arg.arg != "self"],
                        "async": is_async,
                        "complexity": complexity,
                        "decorators": decorators,
                    }
                    if not any(isinstance(parent, ast.ClassDef) for parent in self.scope_stack[:-1]):
                        code_structure["functions"].append(function_info)
                    self.generic_visit(node)
                    self.scope_stack.pop()

                def visit_ClassDef(self, node: ast.ClassDef):
                    """Visits a class definition node."""
                    self.scope_stack.append(node)
                    class_docstring = ast.get_docstring(node) or ""
                    class_info = {
                        "name": node.name,
                        "docstring": class_docstring,
                        "methods": [],
                        "decorators": [ast.unparse(d) for d in node.decorator_list] if hasattr(ast, 'unparse') else []
                    }
                    for body_item in node.body:
                        if isinstance(body_item, (ast.FunctionDef, ast.AsyncFunctionDef)):
                            self.scope_stack.append(body_item)
                            full_method_name = ".".join([scope.name for scope in self.scope_stack if hasattr(scope, 'name')])
                            complexity = function_complexity.get(full_method_name, 0)
                            decorators = [ast.unparse(d) for d in body_item.decorator_list] if hasattr(ast, 'unparse') else []
                            method_docstring = ast.get_docstring(body_item) or ""
                            method_info = {
                                "name": body_item.name,
                                "docstring": method_docstring,
                                "args": [arg.arg for arg in body_item.args.args if arg.arg != "self"],
                                "async": isinstance(body_item, ast.AsyncFunctionDef),
                                "complexity": complexity,
                                "decorators": decorators,
                                "type": "async" if isinstance(body_item, ast.AsyncFunctionDef) else "instance"
                            }
                            class_info["methods"].append(method_info)
                            self.scope_stack.pop()
                    code_structure["classes"].append(class_info)
                    self.generic_visit(node)
                    self.scope_stack.pop()

                def visit_Assign(self, node: ast.Assign):
                    """Processes assignment nodes for extracting variable information."""
                    for target in node.targets:
                        self._process_target(target, node.value)
                    self.generic_visit(node)

                def _process_target(self, target: ast.AST, value: ast.AST) -> None:
                    """Recursively processes assignment targets to extract variable information.

                    Args:
                        target (ast.AST): The assignment target node.
                        value (ast.AST): The value assigned to the target.
                    """
                    if isinstance(target, ast.Name):
                        var_name = target.id
                        is_constant = var_name.isupper()
                        var_type = self._infer_type(value)
                        description = self._extract_description(target.lineno)
                        example = self._extract_example(target.lineno)
                        references = self._extract_references(target.lineno)
                        var_info = {
                            "name": var_name,
                            "type": var_type,
                            "description": description,
                            "file": os.path.basename(self.file_path),
                            "line": target.lineno,
                            "link": f"https://github.com/user/repo/blob/main/{self.file_path}#L{target.lineno}",
                            "example": example,
                            "references": references
                        }
                        if is_constant:
                            code_structure["constants"].append(var_info)
                        else:
                            code_structure["variables"].append(var_info)
                    elif isinstance(target, (ast.Tuple, ast.List)):
                        for elt in target.elts:
                            self._process_target(elt, value)
                    elif isinstance(target, ast.Attribute):
                        var_name = target.attr
                        is_constant = var_name.isupper()
                        var_type = self._infer_type(value)
                        description = self._extract_description(target.lineno)
                        example = self._extract_example(target.lineno)
                        references = self._extract_references(target.lineno)
                        var_info = {
                            "name": var_name,
                            "type": var_type,
                            "description": description,
                            "file": os.path.basename(self.file_path),
                            "line": target.lineno,
                            "link": f"https://github.com/user/repo/blob/main/{self.file_path}#L{target.lineno}",
                            "example": example,
                            "references": references
                        }
                        if is_constant:
                            code_structure["constants"].append(var_info)
                        else:
                            code_structure["variables"].append(var_info)
                    # Handle other target types if necessary

                def visit_With(self, node: ast.With):
                    """Processes 'with' statements."""
                    for item in node.items:
                        if isinstance(item.context_expr, ast.Call):
                            context_manager = ast.unparse(item.context_expr) if hasattr(ast, 'unparse') else ""
                            code_structure.setdefault("context_managers", []).append(context_manager)
                    self.generic_visit(node)

                def visit_AsyncWith(self, node: ast.AsyncWith):
                    """Processes 'async with' statements."""
                    for item in node.items:
                        if isinstance(item.context_expr, ast.Call):
                            context_manager = ast.unparse(item.context_expr) if hasattr(ast, 'unparse') else ""
                            code_structure.setdefault("context_managers", []).append(context_manager)
                    self.generic_visit(node)

                def visit_ListComp(self, node: ast.ListComp):
                    """Tracks list comprehensions."""
                    code_structure.setdefault("comprehensions", []).append("ListComprehension")
                    self.generic_visit(node)

                def visit_DictComp(self, node: ast.DictComp):
                    """Tracks dictionary comprehensions."""
                    code_structure.setdefault("comprehensions", []).append("DictComprehension")
                    self.generic_visit(node)

                def visit_SetComp(self, node: ast.SetComp):
                    """Tracks set comprehensions."""
                    code_structure.setdefault("comprehensions", []).append("SetComprehension")
                    self.generic_visit(node)

                def visit_GeneratorExp(self, node: ast.GeneratorExp):
                    """Tracks generator expressions."""
                    code_structure.setdefault("comprehensions", []).append("GeneratorExpression")
                    self.generic_visit(node)

                # Helper methods for detailed information
                def _infer_type(self, value: ast.AST) -> str:
                    """Infers the type of a variable based on its assigned value.

                    Args:
                        value (ast.AST): The value assigned to the variable.

                    Returns:
                        str: The inferred type as a string.
                    """
                    if isinstance(value, ast.Constant):
                        return type(value.value).__name__
                    elif isinstance(value, ast.List):
                        return "List"
                    elif isinstance(value, ast.Tuple):
                        return "Tuple"
                    elif isinstance(value, ast.Dict):
                        return "Dict"
                    elif isinstance(value, ast.Set):
                        return "Set"
                    elif isinstance(value, ast.Call):
                        return "Call"
                    elif isinstance(value, ast.BinOp):
                        return "BinOp"
                    elif isinstance(value, ast.UnaryOp):
                        return "UnaryOp"
                    elif isinstance(value, ast.Lambda):
                        return "Lambda"
                    elif isinstance(value, ast.Name):
                        return "Name"
                    else:
                        return "Unknown"

                def _extract_description(self, lineno: int) -> str:
                    """Extracts the description of a variable from inline comments.

                    Args:
                        lineno (int): The line number of the variable assignment.

                    Returns:
                        str: The extracted description.
                    """
                    comments = self.comments.get(lineno - 1, []) + self.comments.get(lineno, [])
                    if comments:
                        return " ".join(comments)
                    return "No description provided."

                def _extract_example(self, lineno: int) -> str:
                    """Extracts an example usage of the variable from comments.

                    Args:
                        lineno (int): The line number of the variable assignment.

                    Returns:
                        str: The example usage.
                    """
                    comments = self.comments.get(lineno + 1, [])
                    if comments:
                        return " ".join(comments)
                    return "No example provided."

                def _extract_references(self, lineno: int) -> str:
                    """Extracts references related to the variable from comments.

                    Args:
                        lineno (int): The line number of the variable assignment.

                    Returns:
                        str: The references.
                    """
                    comments = self.comments.get(lineno + 2, [])
                    if comments:
                        return " ".join(comments)
                    return "N/A"

            visitor = CodeVisitor(file_path)
            visitor.visit(tree)
            logger.debug(f"Extracted structure for '{file_path}': {code_structure}")
            return code_structure
        except SyntaxError as e:
            logger.error(f"Syntax error in code: {e.text.strip()} at line {e.lineno}, offset {e.offset}")
            return {}
        except Exception as e:
            logger.error(f"Error extracting Python structure: {e}", exc_info=True)
            return {}

    def insert_docstrings(self, code: str, documentation: Dict[str, Any]) -> str:
        """
        Inserts docstrings into Python code based on the provided documentation.

        Args:
            code (str): The original source code.
            documentation (Dict[str, Any]): Documentation details obtained from AI.

        Returns:
            str: The source code with inserted documentation.
        """
        logger.debug("Starting docstring insertion for Python code.")
        try:
            docstrings_mapping = {}
            for func_doc in documentation.get("functions", []):
                name = func_doc.get("name")
                if name:
                    docstrings_mapping[name] = func_doc.get("docstring", "")
            for class_doc in documentation.get("classes", []):
                class_name = class_doc.get("name")
                if class_name:
                    docstrings_mapping[class_name] = class_doc.get("docstring", "")
                    for method_doc in class_doc.get("methods", []):
                        method_name = method_doc.get("name")
                        if method_name:
                            full_method_name = f"{class_name}.{method_name}"
                            docstrings_mapping[full_method_name] = method_doc.get("docstring", "")

            class DocstringInserter(cst.CSTTransformer):
                def __init__(self, docstrings_mapping: Dict[str, str]):
                    self.docstrings_mapping = docstrings_mapping
                    self.scope_stack = []

                def visit_FunctionDef(self, node: FunctionDef):
                    self.scope_stack.append(node.name.value)

                def leave_FunctionDef(self, original_node: FunctionDef, updated_node: FunctionDef) -> FunctionDef:
                    full_name = ".".join(self.scope_stack)
                    docstring = self.docstrings_mapping.get(full_name)
                    if docstring and not original_node.get_docstring():
                        new_doc = SimpleStatementLine([Expr(SimpleString(f'"""{docstring}"""'))])
                        new_body = [new_doc] + list(updated_node.body.body)
                        updated_node = updated_node.with_changes(body=updated_node.body.with_changes(body=new_body))
                        logger.debug(f"Inserted docstring for function: {full_name}")
                    self.scope_stack.pop()
                    return updated_node

                def visit_ClassDef(self, node: ClassDef):
                    self.scope_stack.append(node.name.value)

                def leave_ClassDef(self, original_node: ClassDef, updated_node: ClassDef) -> ClassDef:
                    full_name = ".".join(self.scope_stack)
                    docstring = self.docstrings_mapping.get(full_name)
                    if docstring and not original_node.get_docstring():
                        new_doc = SimpleStatementLine([Expr(SimpleString(f'"""{docstring}"""'))])
                        new_body = [new_doc] + list(updated_node.body.body)
                        updated_node = updated_node.with_changes(body=updated_node.body.with_changes(body=new_body))
                        logger.debug(f"Inserted docstring for class: {full_name}")
                    self.scope_stack.pop()
                    return updated_node

            tree = cst.parse_module(code)
            inserter = DocstringInserter(docstrings_mapping)
            modified_tree = tree.visit(inserter)
            modified_code = modified_tree.code
            logger.debug("Docstring insertion completed successfully.")
            return modified_code
        except Exception as e:
            logger.error(f"Error inserting docstrings: {e}", exc_info=True)
            return code

    def validate_code(self, code: str, file_path: Optional[str] = None) -> bool:
        """
        Validates the modified Python code for syntax correctness.

        Args:
            code (str): The modified source code.
            file_path (Optional[str]): Path to the Python source file (optional).

        Returns:
            bool: True if the code is valid, False otherwise.
        """
        logger.debug("Starting Python code validation.")
        try:
            ast.parse(code)
            logger.debug("Syntax validation passed.")
            if file_path:
                with tempfile.NamedTemporaryFile(mode="w", suffix=".py", delete=False) as tmp:
                    tmp.write(code)
                    temp_file = tmp.name
                try:
                    result = subprocess.run(
                        ["flake8", temp_file], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
                    )
                    if result.returncode != 0:
                        logger.error(f"Flake8 validation failed for {file_path}:\n{result.stdout}\n{result.stderr}")
                        return False
                    else:
                        logger.debug("Flake8 validation passed.")
                except FileNotFoundError:
                    logger.error("flake8 is not installed or not found in PATH. Please install it using 'pip install flake8'.")
                    return False
                except subprocess.SubprocessError as e:
                    logger.error(f"Subprocess error during flake8 execution: {e}")
                    return False
                finally:
                    try:
                        os.remove(temp_file)
                    except OSError as e:
                        logger.error(f"Error deleting temporary file {temp_file}: {e}")
            else:
                logger.warning("File path not provided for flake8 validation. Skipping flake8.")
            return True
        except SyntaxError as e:
            logger.error(f"Syntax error during validation: {e.text.strip()} at line {e.lineno}, offset {e.offset}")
            return False
        except Exception as e:
            logger.error(f"Unexpected error during code validation: {e}", exc_info=True)
            return False

```

## language_functions/js_ts_handler.py

```python
# language_functions/js_ts_handler.py
import os
import logging
import subprocess
import json
from typing import Dict, Any, Optional
from language_functions.base_handler import BaseHandler

logger = logging.getLogger(__name__)

class JSTsHandler(BaseHandler):
    """Handler for JavaScript and TypeScript languages."""

    def __init__(self, function_schema: Dict[str, Any]):
        """Initializes the JSTsHandler with a function schema."""
        self.function_schema = function_schema

    def extract_structure(self, code: str, file_path: str = None) -> Dict[str, Any]:
        logger.debug("Extracting JS/TS structure from file: %s", file_path)
        try:
            script_path = os.path.join(os.path.dirname(__file__), '..', 'scripts', 'acorn_parser.js')
            input_data = {
                "code": code,
                "language": "javascript"  # or determine based on file_path
            }
            input_json = json.dumps(input_data)
            result = subprocess.run(
                ["node", script_path],
                input=input_json,
                capture_output=True,
                text=True,
                check=True
            )
            structure = json.loads(result.stdout)
            logger.debug("Extracted JS/TS code structure successfully from file: %s", file_path)
            return structure
        except subprocess.CalledProcessError as e:
            logger.error("Error running acorn_parser.js for file %s: %s", file_path, e.stderr)
            return {}
        except json.JSONDecodeError as e:
            logger.error("Error parsing output from acorn_parser.js for file %s: %s", file_path, e)
            return {}
        except Exception as e:
            logger.error("Unexpected error extracting JS/TS structure from file %s: %s", file_path, e)
            return {}

    def insert_docstrings(self, code: str, documentation: Dict[str, Any]) -> str:
        """Inserts JSDoc comments into JS/TS code based on the provided documentation."""
        logger.debug("Inserting JSDoc docstrings into JS/TS code.")
        try:
            script_path = os.path.join(os.path.dirname(__file__), "..", "scripts", "acorn_inserter.js")
            input_data = {
                "code": code,
                "documentation": documentation,
                "language": "javascript"  # or "typescript" based on actual language
            }
            input_json = json.dumps(input_data)
            result = subprocess.run(
                ["node", script_path],
                input=input_json,
                capture_output=True,
                text=True,
                check=True
            )
            modified_code = result.stdout
            logger.debug("Completed inserting JSDoc docstrings into JS/TS code.")
            return modified_code
        except subprocess.CalledProcessError as e:
            logger.error("Error running acorn_inserter.js: %s", e.stderr)
            return code
        except Exception as e:
            logger.error("Unexpected error inserting JSDoc docstrings: %s", e)
            return code

    def validate_code(self, code: str, file_path: Optional[str] = None) -> bool:
        """
        Validates JavaScript/TypeScript code for syntax correctness and style compliance.

        Args:
            code (str): The JS/TS code to validate.
            file_path (Optional[str]): The path to the file being validated.

        Returns:
            bool: True if the code is valid, False otherwise.
        """
        logger.debug('Starting JavaScript/TypeScript code validation for file: %s', file_path)
        if not file_path:
            logger.warning('File path not provided for ESLint validation. Skipping ESLint.')
            return True  # Assuming no linting without a file

        try:
            # Write code to a temporary file for ESLint
            temp_file = f"{file_path}.temp"
            with open(temp_file, 'w', encoding='utf-8') as f:
                f.write(code)

            process = subprocess.run(
                ['eslint', temp_file],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )

            # Remove temporary file
            os.remove(temp_file)

            if process.returncode != 0:
                logger.error('ESLint validation failed for %s:\n%s', file_path, process.stdout)
                return False
            else:
                logger.debug('ESLint validation successful for %s.', file_path)
            return True
        except FileNotFoundError:
            logger.error("ESLint is not installed. Please install it using 'npm install eslint'.")
            return False
        except Exception as e:
            logger.error('Unexpected error during ESLint validation for %s: %s', file_path, e)
            return False
```

## language_functions/cpp_handler.py

```python
# language_functions/cpp_handler.py

import subprocess
import json
import logging
import re
from typing import Dict, Any, Optional  # Import Dict, Any, and Optional
from language_functions.base_handler import BaseHandler

logger = logging.getLogger(__name__)

class CppHandler(BaseHandler):
    def __init__(self, function_schema: Dict[str, Any]):
        """Initializes the PythonHandler with a function schema."""
        self.function_schema = function_schema

    def extract_structure(self, code: str, file_path: str) -> Dict[str, Any]:
        """Extracts structure from C++ code."""
        # Use code and file_path to extract structure
        """Extracts structure from C++ code using libclang."""
        try:
            # Run clang command to get JSON AST
            process = subprocess.run(
                ["clang", "-Xclang", "-ast-dump=json", "-fsyntax-only", file_path],
                capture_output=True,
                text=True,
                check=True
            )
            ast_json = process.stdout
            ast_data = json.loads(ast_json)

            structure = {
                "summary": "",
                "changes_made": [],
                "functions": [],
                "classes": []
            }

            def traverse_ast(node):
                """Recursively traverses the C++ AST."""
                kind = node.get("kind", "")
                if kind == "FunctionDecl":
                    func = {
                        "name": node["name"],
                        "args": [param["name"] for param in node.get("inner", []) if param.get("kind", "") == "ParmVarDecl"],
                        "docstring": extract_docstring_from_comment(node.get("inner", [])).strip(),
                        "async": False  # C++ doesn't have async/await keywords
                    }
                    structure["functions"].append(func)
                elif kind == "CXXRecordDecl" and node.get("tagUsed", "") == "class":
                    cls = {
                        "name": node["name"],
                        "docstring": extract_docstring_from_comment(node.get("inner", [])),
                        "methods": [],
                        "attributes": []
                    }
                    for member in node.get("inner", []):
                        if member.get("kind", "") == "CXXMethodDecl":
                            method = {
                                "name": member["name"],
                                "docstring": extract_docstring_from_comment(member.get("inner", [])),
                                "parameters": [],
                                "return_value": {"type": member.get("type", {}).get("qualType", ""), "description": ""},
                                "examples": [],
                                "error_handling": "",
                                "assumptions_preconditions": ""
                            }
                            for param in member.get("inner", []):
                                if param.get("kind", "") == "ParmVarDecl":
                                    param_type = param.get("type", {}).get("qualType", "")
                                    method["parameters"].append({"name": param["name"], "type": param_type, "description": ""})
                            cls["methods"].append(method)
                        elif member.get("kind", "") == "FieldDecl":
                            attr = {
                                "name": member["name"],
                                "type": member.get("type", {}).get("qualType", ""),
                                "description": extract_docstring_from_comment(member.get("inner", []))
                            }
                            cls["attributes"].append(attr)
                    structure["classes"].append(cls)
                for child in node.get("inner", []):
                    traverse_ast(child)

            traverse_ast(ast_data)
            return structure

        except subprocess.CalledProcessError as e:
            logger.error(f"Error running clang: {e.stderr}")
            return {}
        except Exception as e:
            logger.error(f"Error extracting C++ structure: {e}", exc_info=True)
            return {}

    def insert_docstrings(self, original_code: str, documentation: Dict[str, Any]) -> str:
        """Inserts Doxygen-style docstrings into C++ code."""
        try:
            modified_code = original_code

            for func in documentation.get("functions", []):
                docstring = func.get("docstring", "").strip()
                if docstring:
                    # Regex to find function definition (handles different return types and parameters)
                    pattern = r"(?P<indent>\s*)(?P<return_type>[\w\s\*\&]+)\s+(?P<name>" + re.escape(func["name"]) + r")\s*\((?P<params>[^)]*)\)\s*\{?"
                    match = re.search(pattern, modified_code)
                    if match:
                        indent = match.group("indent")
                        docstring_lines = docstring.splitlines()
                        formatted_docstring = "\n".join([f"{indent}/// {line}" for line in docstring_lines])
                        modified_code = re.sub(pattern, f"{indent}/**\n{formatted_docstring}\n{indent}*/\n{match.group(0)}", modified_code)

            for cls in documentation.get("classes", []):
                docstring = cls.get("docstring", "").strip()
                if docstring:
                    # Regex to find class definition
                    pattern = r"(?P<indent>\s*)class\s+(?P<name>" + re.escape(cls["name"]) + r")\s*\{?"
                    match = re.search(pattern, modified_code)
                    if match:
                        indent = match.group("indent")
                        docstring_lines = docstring.splitlines()
                        formatted_docstring = "\n".join([f"{indent}/// {line}" for line in docstring_lines])
                        modified_code = re.sub(pattern, f"{indent}/**\n{formatted_docstring}\n{indent}*/\n{match.group(0)}", modified_code)

                for method in cls.get("methods", []):
                    docstring = method.get("docstring", "").strip()
                    if docstring:
                        # Regex to find method definition (within the class)
                        pattern = r"(?P<indent>\s*)(?P<return_type>[\w\s\*\&]+)\s+(?P<class_name>" + re.escape(cls["name"]) + r")::(?P<name>" + re.escape(method["name"]) + r")\s*\((?P<params>[^)]*)\)\s*\{?"
                        match = re.search(pattern, modified_code)
                        if match:
                            indent = match.group("indent")
                            docstring_lines = docstring.splitlines()
                            formatted_docstring = "\n".join([f"{indent}/// {line}" for line in docstring_lines])
                            modified_code = re.sub(pattern, f"{indent}/**\n{formatted_docstring}\n{indent}*/\n{match.group(0)}", modified_code)

            return modified_code

        except Exception as e:
            logger.error(f"Error inserting C++ docstrings: {e}", exc_info=True)
            return original_code

    def validate_code(self, code: str, file_path: Optional[str] = None) -> bool:
        """
        Validates C++ code by attempting to compile it with syntax-only check.

        Args:
            code (str): The C++ code to validate.
            file_path (Optional[str]): The path to the C++ file being validated.

        Returns:
            bool: True if the code compiles successfully, False otherwise.
        """
        logger.debug('Starting C++ code validation.')
        if not file_path:
            logger.warning('File path not provided for C++ validation. Skipping compilation.')
            return True  # Assuming no compilation without a file

        try:
            # Write code to the specified file path
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(code)

            # Attempt to compile the C++ file with syntax-only check
            process = subprocess.run(
                ['g++', '-fsyntax-only', file_path],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )

            if process.returncode != 0:
                logger.error(f'C++ compilation failed for {file_path}:\n{process.stderr}')
                return False
            else:
                logger.debug('C++ compilation successful.')
            return True
        except FileNotFoundError:
            logger.error("C++ compiler (g++) not found. Please ensure g++ is installed and in the PATH.")
            return False
        except Exception as e:
            logger.error(f'Unexpected error during C++ code validation: {e}')
            return False

def extract_docstring_from_comment(nodes):
    """Extracts docstring from comment nodes in the AST."""
    for node in nodes:
        if node.get("kind", "") == "FullComment":
            comment = node.get("text", "").strip()
            if comment.startswith("/**") and comment.endswith("*/"):
                return comment[3:-2].strip()
    return ""

```

## language_functions/go_handler.py

```python
# language_functions/go_handler.py

import subprocess
import json
import logging
import re
from typing import Dict, Any, Optional
from language_functions.base_handler import BaseHandler

logger = logging.getLogger(__name__)

class GoHandler(BaseHandler):
    def __init__(self, function_schema):
        self.function_schema = function_schema

    def extract_structure(self, code: str, file_path: str) -> Dict[str, Any]:

        try:
            process = subprocess.run(
                ["go", "ast", "-json"],
                input=code,
                capture_output=True,
                text=True,
                check=True
            )
            ast_json = process.stdout
            ast_data = json.loads(ast_json)

            structure = {
                "summary": "",
                "changes_made": [],
                "functions": [],
                "interfaces": [],  # Go interfaces
                "structs": []       # Go structs
            }

            def traverse_ast(node):
                """Recursively traverses the Go AST."""
                if node["Kind"] == "FuncDecl":
                    func = {
                        "name": node["Name"],
                        "args": [param.get("Names", [{}])[0].get("Name", "") for param in node.get("Type", {}).get("Params", {}).get("List", [])],
                        "docstring": node.get("Doc", {}).get("Text", "").strip(),
                        "async": False  # Go doesn't have async/await keywords
                    }
                    structure["functions"].append(func)
                elif node["Kind"] == "InterfaceType":
                    interface = {
                        "name": node["Name"],
                        "docstring": "",
                        "methods": []
                    }
                    if "Doc" in node and "Text" in node["Doc"]:
                        interface["docstring"] = node["Doc"]["Text"].strip()
                    for method in node.get("Methods", {}).get("List", []):
                        method_name = method["Names"][0]["Name"]
                        method_type = get_type_string(method["Type"])
                        interface["methods"].append({"name": method_name, "type": method_type, "description": ""})
                    structure["interfaces"].append(interface)
                elif node["Kind"] == "StructType":
                    struct = {
                        "name": node["Name"],
                        "docstring": "",
                        "fields": []
                    }
                    if "Doc" in node and "Text" in node["Doc"]:
                        struct["docstring"] = node["Doc"]["Text"].strip()
                    for field in node.get("Fields", {}).get("List", []):
                        field_name = field["Names"][0]["Name"]
                        field_type = get_type_string(field["Type"])
                        struct["fields"].append({"name": field_name, "type": field_type, "description": ""})
                    structure["structs"].append(struct)
                for child in node.values():
                    if isinstance(child, dict):
                        traverse_ast(child)
                    elif isinstance(child, list):
                        for item in child:
                            if isinstance(item, dict):
                                traverse_ast(item)

            traverse_ast(ast_data)
            return structure

        except subprocess.CalledProcessError as e:
            logger.error(f"Error running 'go ast': {e.stderr}")
            return {}
        except Exception as e:
            logger.error(f"Error extracting Go structure: {e}", exc_info=True)
            return {}

    def insert_docstrings(self, code: str, documentation: Dict[str, Any]) -> str:
        """Inserts docstrings into Go code."""
        try:
            modified_code = code
            for func in documentation.get("functions", []):
                docstring = func.get("docstring", "").strip()
                if docstring:
                    pattern = r"(?P<indent>\s*)func\s+(?P<name>" + re.escape(func["name"]) + r")\s*\((?P<params>[^)]*)\)\s*(?P<return_type>.*?)\s*\{?"
                    match = re.search(pattern, modified_code)
                    if match:
                        indent = match.group("indent")
                        docstring_lines = docstring.splitlines()
                        formatted_docstring = "\n".join([f"{indent}// {line}" for line in docstring_lines])
                        modified_code = re.sub(pattern, f"{indent}{formatted_docstring}\n{match.group(0)}", modified_code)
            return modified_code

        except Exception as e:
            logger.error(f"Error inserting Go docstrings: {e}", exc_info=True)
            return code

    def validate_code(self, code: str) -> bool:
        """Validates Go code using the 'go vet' command."""
        try:
            process = subprocess.run(["go", "vet"], input=code, capture_output=True, text=True, check=False)
            if process.returncode == 0:
                logger.debug("Go code validation successful.")
                return True
            else:
                logger.error(f"Go code validation failed:\n{process.stderr}")
                return False
        except FileNotFoundError:
            logger.error("Go compiler not found. Please install Go.")
            return False
        except Exception as e:
            logger.error(f"Unexpected error during Go code validation: {e}")
            return False

def get_type_string(type_node):
    """Extracts the type string from a Go AST type node."""
    if isinstance(type_node, str):
        return type_node
    elif isinstance(type_node, dict):
        return type_node.get("Name", "") or get_type_string(type_node.get("Type", ""))
    return ""

```

## language_functions/css_handler.py

```python
# language_functions/css_handler.py

import logging
import subprocess  # Ensure subprocess is imported
from typing import Dict, Any, Optional
import tinycss2
from language_functions.base_handler import BaseHandler

logger = logging.getLogger(__name__)

class CSSHandler:
    """Handler for CSS language."""

    def __init__(self, function_schema: Dict[str, Any]):
        """Initializes the CSSHandler with a function schema."""
        self.function_schema = function_schema

    def extract_structure(self, code: str, file_path: str) -> Dict[str, Any]:
        """
        Extracts structure from CSS code.

        Args:
            code (str): The CSS source code to parse.
            file_path (str): The path to the CSS file.

        Returns:
            dict: A dictionary containing the structure of the CSS code.
        """
        logger.debug("Extracting CSS structure.")
        try:
            rules = tinycss2.parse_rule_list(code, skip_whitespace=True, skip_comments=True)
            structure = {"rules": []}
            for rule in rules:
                if rule.type == "qualified-rule":
                    selectors = "".join([token.serialize() for token in rule.prelude]).strip()
                    declarations = []
                    for decl in tinycss2.parse_declaration_list(rule.content):
                        if decl.type == "declaration":
                            declarations.append({
                                "property": decl.lower_name,
                                "value": "".join([token.serialize() for token in decl.value]).strip(),
                            })
                    structure["rules"].append({
                        "selectors": selectors,
                        "declarations": declarations
                    })
                    logger.debug("Extracted rule: %s", selectors)
            return structure
        except tinycss2.ParseError as e:
            logger.error("Parse error while extracting CSS structure: %s", e)
            return {}
        except Exception as e:
            logger.error("Unexpected error while extracting CSS structure: %s", e)
            return {}

    def insert_docstrings(self, code: str, documentation: Dict[str, Any]) -> str:
        """Inserts comments into CSS code based on provided documentation."""
        logger.debug("Inserting CSS docstrings.")
        try:
            summary = documentation.get("summary", "").strip()
            changes = documentation.get("changes_made", [])
            if not summary and not changes:
                logger.warning("No summary or changes provided in documentation. Skipping comment insertion.")
                return code
            new_comment_parts = []
            if summary:
                new_comment_parts.append(f"Summary: {summary}")
            if changes:
                changes_formatted = "; ".join(changes)
                new_comment_parts.append(f"Changes: {changes_formatted}")
            new_comment = "/* " + " | ".join(new_comment_parts) + " */\n"
            modified_code = new_comment + code
            logger.debug("Completed inserting CSS docstrings.")
            return modified_code
        except Exception as e:
            logger.error("Error inserting CSS docstrings: %s", e)
            return code

    def validate_code(self, code: str, file_path: Optional[str] = None) -> bool:
        """
        Validates CSS code using Stylelint.

        Args:
            code (str): The CSS code to validate.
            file_path (Optional[str]): The path to the CSS file being validated.

        Returns:
            bool: True if the code is valid, False otherwise.
        """
        logger.debug('Starting CSS code validation for file: %s', file_path)
        if not file_path:
            logger.warning('File path not provided for Stylelint validation. Skipping validation.')
            return True  # Assuming no validation without a file

        try:
            # Write code to the specified file path
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(code)

            # Attempt to validate the CSS file using Stylelint
            process = subprocess.run(
                ['stylelint', file_path],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )

            if process.returncode != 0:
                logger.error('Stylelint validation failed for %s:\n%s', file_path, process.stdout)
                return False
            else:
                logger.debug('Stylelint validation successful for %s.', file_path)
            return True
        except FileNotFoundError:
            logger.error("Stylelint not found. Please install it using 'npm install -g stylelint'.")
            return False
        except Exception as e:
            logger.error('Unexpected error during Stylelint validation for %s: %s', file_path, e)
            return False
```

## language_functions/java_handler.py

```python
import os
import javalang
import logging
import subprocess
from typing import Dict, Any, Optional
from language_functions.base_handler import BaseHandler

logger = logging.getLogger(__name__)

class JavaHandler(BaseHandler):
    def __init__(self, function_schema):
        self.function_schema = function_schema

    # Other methods...
    """Handler for Java language."""

    def extract_structure(self, code: str) -> Dict[str, Any]:
        """Parses Java code to extract classes and methods."""
        logger.debug("Extracting Java code structure.")
        
        structure = {
            "classes": [],
            "functions": [],
        }
        
        try:
            tokens = javalang.tokenizer.tokenize(code)
            parser = javalang.parser.Parser(tokens)
            tree = parser.parse()
            
            for path, node in tree.filter(javalang.tree.ClassDeclaration):
                cls = {
                    "name": node.name,
                    "methods": [],
                    "docstring": node.documentation or ""
                }
                
                for method in node.methods:
                    func = {
                        "name": method.name,
                        "args": [param.name for param in method.parameters],
                        "docstring": method.documentation or "",
                    }
                    cls["methods"].append(func)
                
                structure["classes"].append(cls)
            
            for path, node in tree.filter(javalang.tree.MethodDeclaration):
                func = {
                    "name": node.name,
                    "args": [param.name for param in node.parameters],
                    "docstring": node.documentation or "",
                }
                structure["functions"].append(func)
            
            return structure
        
        except javalang.parser.JavaSyntaxError as e:
            logger.error(f"Failed to parse Java code: {e}")
            return {}
        except Exception as e:
            logger.error(f"Unexpected error during Java code parsing: {e}")
            return {}

    def insert_docstrings(self, code: str, documentation: Dict[str, Any]) -> str:
        """Inserts Javadoc comments into Java code based on the provided documentation."""
        logger.debug("Inserting Javadoc docstrings into Java code.")
        
        for class_doc in documentation.get("classes", []):
            class_name = class_doc.get("name")
            class_docstring = class_doc.get("docstring", "")
            code = self.insert_comment(code, class_name, class_docstring)
            
            for method_doc in class_doc.get("methods", []):
                method_name = method_doc.get("name")
                method_docstring = method_doc.get("docstring", "")
                code = self.insert_comment(code, method_name, method_docstring)
        
        return code

    def insert_comment(self, code: str, name: str, comment: str) -> str:
        """Inserts comment above the specified class/method name in the code."""
        comment_block = f"/**\n * {comment}\n */\n"
        code = code.replace(f"class {name}", f"{comment_block}class {name}")
        code = code.replace(f"void {name}", f"{comment_block}void {name}")
        return code

    def validate_code(self, code: str, file_path: Optional[str] = None) -> bool:
        """
        Validates Java code by attempting to compile it.

        Args:
            code (str): The Java code to validate.
            file_path (Optional[str]): The path to the Java file being validated.

        Returns:
            bool: True if the code compiles successfully, False otherwise.
        """
        logger.debug('Starting Java code validation.')
        if not file_path:
            logger.warning('File path not provided for Java validation. Skipping compilation.')
            return True  # Assuming no compilation without a file

        try:
            # Write code to the specified file path
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(code)

            # Attempt to compile the Java file
            process = subprocess.run(
                ['javac', file_path],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )

            if process.returncode != 0:
                logger.error(f'Java compilation failed for {file_path}:\n{process.stderr}')
                return False
            else:
                logger.debug('Java compilation successful.')
                # Optionally, remove the .class file after validation
                class_file = file_path.replace('.java', '.class')
                if os.path.exists(class_file):
                    os.remove(class_file)
                    logger.debug(f'Removed compiled class file {class_file}.')
            return True
        except FileNotFoundError:
            logger.error("Java compiler (javac) not found. Please ensure JDK is installed and javac is in the PATH.")
            return False
        except Exception as e:
            logger.error(f'Unexpected error during Java code validation: {e}')
            return False
```

## language_functions/__init__.py

```python
# language_functions/__init__.py

import logging
from typing import Dict, Any, Optional

from .python_handler import PythonHandler
from .java_handler import JavaHandler
from .js_ts_handler import JSTsHandler
from .go_handler import GoHandler
from .cpp_handler import CppHandler
from .html_handler import HTMLHandler
from .css_handler import CSSHandler
from .base_handler import BaseHandler

logger = logging.getLogger(__name__)

def get_handler(language: str, function_schema: Dict[str, Any]) -> Optional[BaseHandler]:
    """
    Factory function to retrieve the appropriate language handler.

    Args:
        language (str): The programming language.
        function_schema (Dict[str, Any]): The schema defining functions.

    Returns:
        Optional[BaseHandler]: The corresponding language handler or None if unsupported.
    """
    if function_schema is None:
        logger.error("Function schema is None. Cannot retrieve handler.")
        return None

    language = language.lower()
    if language == 'python':
        return PythonHandler(function_schema)
    elif language == 'java':
        return JavaHandler(function_schema)
    elif language in ['javascript', 'js', 'typescript', 'ts']:
        return JSTsHandler(function_schema)
    elif language == 'go':
        return GoHandler(function_schema)
    elif language in ['cpp', 'c++', 'cxx']:
        return CppHandler(function_schema)
    elif language in ['html', 'htm']:
        return HTMLHandler(function_schema)
    elif language == 'css':
        return CSSHandler(function_schema)
    else:
        logger.warning(f"No handler available for language: {language}")
        return None

```

## schemas/function_schema.json

```
{
  "functions": [
    {
      "name": "generate_documentation",
      "description": "Generates documentation for code structures.",
      "parameters": {
        "type": "object",
        "properties": {
          "summary": {
            "type": "string",
            "description": "A detailed summary of the file."
          },
          "changes_made": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "description": "List of changes made to the file."
          },
          "functions": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": {
                "name": {
                  "type": "string",
                  "description": "Function name."
                },
                "docstring": {
                  "type": "string",
                  "description": "Detailed description of the function."
                },
                "args": {
                  "type": "array",
                  "items": {
                    "type": "string"
                  },
                  "description": "List of argument names."
                },
                "async": {
                  "type": "boolean",
                  "description": "Whether the function is asynchronous."
                }
              },
              "required": ["name", "docstring", "args", "async"]
            },
            "description": "List of functions."
          },
          "classes": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": {
                "name": {
                  "type": "string",
                  "description": "Class name."
                },
                "docstring": {
                  "type": "string",
                  "description": "Detailed description of the class."
                },
                "methods": {
                  "type": "array",
                  "items": {
                    "type": "object",
                    "properties": {
                      "name": {
                        "type": "string",
                        "description": "Method name."
                      },
                      "docstring": {
                        "type": "string",
                        "description": "Detailed description of the method."
                      },
                      "args": {
                        "type": "array",
                        "items": {
                          "type": "string"
                        },
                        "description": "List of argument names."
                      },
                      "async": {
                        "type": "boolean",
                        "description": "Whether the method is asynchronous."
                      },
                      "type": {
                        "type": "string",
                        "description": "Method type (e.g., 'instance', 'class', 'static')."
                      }
                    },
                    "required": ["name", "docstring", "args", "async", "type"]
                  },
                  "description": "List of methods."
                }
              },
              "required": ["name", "docstring", "methods"]
            },
            "description": "List of classes."
          }
        },
        "required": ["summary", "changes_made", "functions", "classes"]
      }
    }
  ]
}

```

## schemas/js_ts_structure_schema.json

```
{
  "$id": "https://example.com/js-ts-structure.schema.json",
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "properties": {
    "functions": {
      "type": "array",
      "items": { "$ref": "#/definitions/function" }
    },
    "classes": {
      "type": "array",
      "items": { "$ref": "#/definitions/class" }
    }
  },
  "required": ["functions", "classes"],
  "definitions": {
    "function": {
      "type": "object",
      "properties": {
        "name": { "type": "string" },
        "args": {
          "type": "array",
          "items": { "type": "string" }
        },
        "async": { "type": "boolean" },
        "docstring": {
          "type": ["string", "null"]
        }
      },
      "required": ["name", "args", "async"]
    },
    "class": {
      "type": "object",
      "properties": {
        "name": { "type": "string" },
        "methods": {
          "type": "array",
          "items": { "$ref": "#/definitions/method" }
        },
        "docstring": {
          "type": ["string", "null"]
        }
      },
      "required": ["name", "methods"]
    },
    "method": {
      "type": "object",
      "properties": {
        "name": { "type": "string" },
        "args": {
          "type": "array",
          "items": { "type": "string" }
        },
        "async": { "type": "boolean" },
        "kind": { "type": "string" },
        "docstring": {
          "type": ["string", "null"]
        }
      },
      "required": ["name", "args", "async", "kind"]
    }
  }
}
```

## schemas/method_schema.json

```
{
  "$id": "https://example.com/method.schema.json",
  "type": "object",
  "properties": {
    "name": {
      "type": "string",
      "description": "The name of the method."
    },
    "args": {
      "type": "array",
      "items": {
        "type": "string"
      },
      "description": "List of arguments for the method."
    },
    "docstring": {
      "type": "string",
      "description": "The docstring or description of the method."
    },
    "async": {
      "type": "boolean",
      "description": "Indicates whether the method is asynchronous."
    },
    "kind": {
      "type": "string",
      "description": "The kind of method (e.g., 'method', 'constructor', 'get', 'set')."
    }
  },
  "required": ["name", "args", "async", "kind"]
}

```

## schemas/html_structure_schema.json

```
{
  "$id": "https://example.com/html-structure.schema.json",
  "type": "object",
  "properties": {
    "tags": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "name": { "type": "string" },
          "attributes": {
            "type": "object",
            "additionalProperties": { "type": "string" }
          }
        },
        "required": ["name", "attributes"]
      }
    }
  },
  "required": ["tags"]
}

```

## schemas/css_schema.json

```
{
  "$id": "https://example.com/css-structure.schema.json",
  "type": "object",
  "properties": {
    "rules": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "selectors": { "type": "string" },
          "declarations": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": {
                "property": { "type": "string" },
                "value": { "type": "string" }
              },
              "required": ["property", "value"]
            }
          }
        },
        "required": ["selectors", "declarations"]
      }
    }
  },
  "required": ["rules"]
}

```

## schemas/documentation_report.json

```
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "DocumentationReport",
  "type": "object",
  "properties": {
    "file": {
      "type": "string",
      "description": "Name of the file being documented."
    },
    "language": {
      "type": "string",
      "description": "Programming language of the file."
    },
    "summary": {
      "type": "string",
      "description": "A brief summary of the file's purpose and functionality."
    },
    "changes_made": {
      "type": "array",
      "description": "List of changes made to the file.",
      "items": {
        "type": "string"
      }
    },
    "code_snippet": {
      "type": "string",
      "description": "Relevant code snippet from the file."
    }
  },
  "required": ["file", "language", "summary", "changes_made", "code_snippet"]
}

```

## schemas/functions_schema.json

```
{
  "$id": "https://example.com/function.schema.json",
  "type": "object",
  "properties": {
    "name": {
      "type": "string",
      "description": "The name of the function."
    },
    "args": {
      "type": "array",
      "items": {
        "type": "string"
      },
      "description": "List of arguments for the function."
    },
    "docstring": {
      "type": "string",
      "description": "The docstring or description of the function."
    },
    "async": {
      "type": "boolean",
      "description": "Indicates whether the function is asynchronous."
    }
  },
  "required": ["name", "args", "async"]
}

```

## schemas/class_schema.json

```
{
  "$id": "https://example.com/class.schema.json",
  "type": "object",
  "properties": {
    "name": {
      "type": "string",
      "description": "The name of the class."
    },
    "methods": {
      "type": "array",
      "items": {
        "$ref": "https://example.com/method.schema.json"
      },
      "description": "List of methods defined in the class with their details."
    },
    "docstring": {
      "type": "string",
      "description": "The docstring or description of the class."
    }
  },
  "required": ["name", "methods"]
}

```

## schemas/python_structure_schema.json

```
{
  "$id": "https://example.com/python-structure.schema.json",
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "properties": {
    "functions": {
      "type": "array",
      "items": { "$ref": "#/definitions/function" }
    },
    "classes": {
      "type": "array",
      "items": { "$ref": "#/definitions/class" }
    }
  },
  "required": ["functions", "classes"],
  "definitions": {
    "function": {
      "type": "object",
      "properties": {
        "name": { "type": "string" },
        "args": {
          "type": "array",
          "items": { "type": "string" }
        },
        "docstring": {
          "type": ["string", "null"]
        },
        "async": { "type": "boolean" }
      },
      "required": ["name", "args", "async"]
    },
    "class": {
      "type": "object",
      "properties": {
        "name": { "type": "string" },
        "methods": {
          "type": "array",
          "items": { "$ref": "#/definitions/method" }
        },
        "docstring": {
          "type": ["string", "null"]
        }
      },
      "required": ["name", "methods"]
    },
    "method": {
      "type": "object",
      "properties": {
        "name": { "type": "string" },
        "args": {
          "type": "array",
          "items": { "type": "string" }
        },
        "docstring": {
          "type": ["string", "null"]
        },
        "async": { "type": "boolean" },
        "type": { "type": "string" }
      },
      "required": ["name", "args", "async", "type"]
    }
  }
}
```

## scripts/acorn_parser.js

```javascript
const fs = require('fs');
const { parse } = require('@typescript-eslint/typescript-estree');
const Ajv = require('ajv');
const path = require('path');

const ajv = new Ajv({ allErrors: true, strict: false });

// Read the schema file
const schemaPath = path.join(__dirname, '../schemas/js_ts_structure_schema.json');
const functionSchema = JSON.parse(fs.readFileSync(schemaPath, 'utf-8'));

// Read data from stdin
let inputChunks = [];
process.stdin.on('data', chunk => {
  inputChunks.push(chunk);
});

process.stdin.on('end', () => {
  const inputData = inputChunks.join('');

  // Remove comments from JSON input
  const sanitizedInput = inputData.replace(/\/\/[^\n]*\n/g, '');

  let parsedInput;
  try {
    parsedInput = JSON.parse(sanitizedInput);
  } catch (e) {
    console.error('Error parsing input JSON: %s', e.message);
    console.error('Input JSON: %s', sanitizedInput);
    process.exit(1);
  }

  const { code, language } = parsedInput;

  // Parse the code
  let ast;
  try {
    ast = parse(code, {
      loc: true,
      range: true,
      comment: true,
      tokens: true,
      errorOnUnknownASTType: false,
      jsx: true,
    });
  } catch (e) {
    console.error('Parsing error: %s', e.message);
    process.exit(1);
  }

  // Function to extract docstrings from leading comments
  function extractDocstring(node) {
    if (node.leadingComments && node.leadingComments.length > 0) {
      const lastComment = node.leadingComments[node.leadingComments.length - 1];
      return lastComment.value.trim();
    }
    return null;
  }

  // Extract functions and classes from the AST
  const structure = {
    functions: [],
    classes: [],
  };

  function traverse(node) {
    switch (node.type) {
      case 'FunctionDeclaration':
        const func = {
          name: node.id ? node.id.name : 'anonymous',
          args: node.params.map(param => (param.name ? param.name : 'param')),
          async: node.async || false,
          docstring: extractDocstring(node),
        };
        structure.functions.push(func);
        break;
      case 'ClassDeclaration':
        const cls = {
          name: node.id ? node.id.name : 'anonymous',
          docstring: extractDocstring(node),
          methods: [],
        };
        if (node.body && node.body.body) {
          node.body.body.forEach(element => {
            if (element.type === 'MethodDefinition') {
              const method = {
                name: element.key.name,
                args: element.value.params.map(param =>
                  param.name ? param.name : 'param'
                ),
                async: element.value.async || false,
                kind: element.kind,
                docstring: extractDocstring(element),
              };
              cls.methods.push(method);
            }
          });
        }
        structure.classes.push(cls);
        break;
      default:
        break;
    }

    // Recurse on child nodes
    for (const key in node) {
      if (node.hasOwnProperty(key)) {
        const child = node[key];
        if (Array.isArray(child)) {
          child.forEach(c => {
            if (c && typeof c.type === 'string') {
              traverse(c);
            }
          });
        } else if (child && typeof child.type === 'string') {
          traverse(child);
        }
      }
    }
  }

  traverse(ast);

  // Perform schema validation
  try {
    const validate = ajv.compile(functionSchema);
    const valid = validate(structure);

    if (!valid) {
      console.error('Validation errors: %s', JSON.stringify(validate.errors, null, 2));
      process.exit(1);
    }
  } catch (e) {
    console.error('Schema validation error: %s', e.message);
    process.exit(1);
  }

  // Output the extracted structure as JSON
  console.log(JSON.stringify(structure, null, 2));
});
```

## scripts/package.json

```
{
  "name": "scripts",
  "version": "1.0.0",
  "main": "extract_structure.js",
  "scripts": {
    "test": "echo \"Error: no test specified\" && exit 1",
    "acorn-parser": "node acorn_parser.js",
    "acorn-inserter": "node acorn_inserter.js"
  },
  "keywords": [],
  "author": "",
  "license": "ISC",
  "dependencies": {
    "@babel/generator": "^7.25.7",
    "@babel/traverse": "^7.25.7",
    "@typescript-eslint/typescript-estree": "^8.8.1",
    "acorn": "^8.12.1",
    "acorn-typescript": "^1.0.0",
    "acorn-walk": "^8.3.4",
    "ajv": "^8.17.1",
    "astring": "^1.9.0",
    "babel": "^5.8.38",
    "prettier": "^3.3.3",
    "recast": "^0.23.9",
    "stylelint": "^16.10.0",
    "tslint": "^5.20.1"
  },
  "description": "",
  "devDependencies": {
    "@typescript-eslint/eslint-plugin": "^8.8.1",
    "@typescript-eslint/parser": "^8.8.1",
    "eslint": "^9.12.0"
  }
}

```

## scripts/acorn_inserter.js

```javascript
// jsdoc_inserter.js

const fs = require('fs');
const { parse } = require('@typescript-eslint/typescript-estree');
const traverse = require('@babel/traverse').default;
const generate = require('@babel/generator').default;

let inputChunks = [];
process.stdin.on('data', chunk => {
  inputChunks.push(chunk);
});

process.stdin.on('end', () => {
  const inputData = JSON.parse(inputChunks.join(''));
  const { code, documentation, language } = inputData;

  // Parse the code
  let ast;
  try {
    ast = parse(code, {
      loc: true,
      range: true,
      comment: true,
      tokens: true,
      errorOnUnknownASTType: false,
      jsx: true,
      useJSXTextNode: true,
      ecmaVersion: 2020,
      sourceType: 'module',
      project: './tsconfig.json', // Ensure this points to your tsconfig.json if using TypeScript
      tsconfigRootDir: __dirname,
    });
  } catch (e) {
    console.error('Parsing error: %s', e.message);
    process.exit(1);
  }

  if (!ast || ast.type !== 'Program') {
    console.error('Error traversing AST: Couldn\'t find a Program');
    process.exit(1);
  }

  // Build a map of function and class names to their docstrings
  const docstringsMapping = {};

  // Process functions
  if (documentation.functions) {
    documentation.functions.forEach(funcDoc => {
      if (funcDoc.name && funcDoc.docstring) {
        // Generate JSDoc format
        const jsDoc = generateJSDoc(funcDoc);
        docstringsMapping[funcDoc.name] = jsDoc;
      }
    });
  }

  // Process classes and their methods
  if (documentation.classes) {
    documentation.classes.forEach(classDoc => {
      if (classDoc.name && classDoc.docstring) {
        const classJsDoc = generateJSDoc(classDoc, true); // true indicates class
        docstringsMapping[classDoc.name] = classJsDoc;
      }
      if (classDoc.methods) {
        classDoc.methods.forEach(methodDoc => {
          if (methodDoc.name && methodDoc.docstring) {
            const fullName = `${classDoc.name}.${methodDoc.name}`;
            const methodJsDoc = generateJSDoc(methodDoc);
            docstringsMapping[fullName] = methodJsDoc;
          }
        });
      }
    });
  }

  // Function to generate JSDoc formatted comments
  function generateJSDoc(doc, isClass = false) {
    let jsDoc = '/**\n';
    jsDoc += ` * ${doc.docstring}\n`;
    if (doc.args && doc.args.length > 0) {
      doc.args.forEach(arg => {
        jsDoc += ` * @param {type} ${arg} - Description.\n`; // Replace 'type' and 'Description' as needed
      });
    }
    if (doc.returns) {
      jsDoc += ` * @returns {type} Description.\n`; // Replace 'type' and 'Description' as needed
    }
    if (isClass) {
      jsDoc += ` * @class\n`;
    }
    jsDoc += ' */\n';
    return jsDoc;
  }

  // Function to insert JSDoc comments
  function insertJSDoc(node, jsDoc) {
    const comment = {
      type: 'CommentBlock',
      value: jsDoc.replace('/**', '').replace('*/', '').trim(),
    };
    if (!node.leadingComments) {
      node.leadingComments = [];
    }
    node.leadingComments.push(comment);
  }

  // Traverse the AST and insert JSDoc comments
  try {
    traverse(ast, {
      enter(path) {
        const node = path.node;
        if (node.type === 'FunctionDeclaration' || node.type === 'FunctionExpression') {
          const name = node.id ? node.id.name : 'anonymous';
          if (docstringsMapping[name]) {
            insertJSDoc(node, docstringsMapping[name]);
          }
        } else if (node.type === 'ClassDeclaration') {
          const className = node.id ? node.id.name : 'anonymous';
          if (docstringsMapping[className]) {
            insertJSDoc(node, docstringsMapping[className]);
          }
          if (node.body && node.body.body) {
            node.body.body.forEach(element => {
              if (element.type === 'MethodDefinition') {
                const methodName = element.key.name;
                const fullName = `${className}.${methodName}`;
                if (docstringsMapping[fullName]) {
                  insertJSDoc(element, docstringsMapping[fullName]);
                }
              }
            });
          }
        } else if (node.type === 'Literal') {
          // Handle Literal node type
          console.log('Literal node found:', node);
        } else if (node.type === 'Property') {
          // Handle Property node type
          console.log('Property node found:', node);
        }
      },
    });
  } catch (e) {
    console.error('Error traversing AST: %s', e.message);
    process.exit(1);
  }

  // Generate code from modified AST, including comments
  let modifiedCode;
  try {
    const result = generate(ast, { comments: true });
    modifiedCode = result.code;
  } catch (e) {
    console.error('Error generating code from AST: %s', e.message);
    process.exit(1);
  }

  // Output the modified code
  console.log(modifiedCode);
});
```

## scripts/.eslintrc.json

```
{
    "env": {
        "browser": true,
        "es2021": true
    },
    "extends": [
        "eslint:recommended",
        "plugin:@typescript-eslint/recommended"
    ],
    "parser": "@typescript-eslint/parser",
    "parserOptions": {
        "ecmaVersion": 12,
        "sourceType": "module"
    },
    "plugins": [
        "@typescript-eslint"
    ],
    "rules": {
        // Add your custom rules here
    }
}

```

## scripts/javadoc_inserter.js

```javascript
// javadoc_inserter.js

const fs = require('fs');
const { parse } = require('@typescript-eslint/typescript-estree'); // Use appropriate parser for Java
const traverse = require('@babel/traverse').default;
const generate = require('@babel/generator').default;

let inputChunks = [];
process.stdin.on('data', chunk => {
  inputChunks.push(chunk);
});

process.stdin.on('end', () => {
  const inputData = JSON.parse(inputChunks.join(''));
  const { code, documentation, language } = inputData;

  // Parse the code
  let ast;
  try {
    ast = parse(code, {
      loc: true,
      range: true,
      comment: true,
      tokens: true,
      errorOnUnknownASTType: false,
      jsx: true,
    });
  } catch (e) {
    console.error(`Parsing error: ${e.message}`);
    process.exit(1);
  }

  // Build a map of function and class names to their docstrings
  const docstringsMapping = {};

  // Process functions
  if (documentation.functions) {
    documentation.functions.forEach(funcDoc => {
      if (funcDoc.name && funcDoc.docstring) {
        // Generate Javadoc format
        const javadoc = generateJavadoc(funcDoc);
        docstringsMapping[funcDoc.name] = javadoc;
      }
    });
  }

  // Process classes and their methods
  if (documentation.classes) {
    documentation.classes.forEach(classDoc => {
      if (classDoc.name && classDoc.docstring) {
        const classJavadoc = generateJavadoc(classDoc, true); // true indicates class
        docstringsMapping[classDoc.name] = classJavadoc;
      }
      if (classDoc.methods) {
        classDoc.methods.forEach(methodDoc => {
          if (methodDoc.name && methodDoc.docstring) {
            const fullName = `${classDoc.name}.${methodDoc.name}`;
            const methodJavadoc = generateJavadoc(methodDoc);
            docstringsMapping[fullName] = methodJavadoc;
          }
        });
      }
    });
  }

  // Function to generate Javadoc formatted comments
  function generateJavadoc(doc, isClass = false) {
    let javadoc = "/**\n";
    javadoc += ` * ${doc.docstring}\n`;
    if (doc.args && doc.args.length > 0) {
      doc.args.forEach(arg => {
        javadoc += ` * @param ${arg} Description.\n`; // Replace 'Description' as needed
      });
    }
    if (doc.returns) {
      javadoc += ` * @return Description.\n`; // Replace 'Description' as needed
    }
    if (isClass) {
      javadoc += ` * @class\n`;
    }
    javadoc += " */\n";
    return javadoc;
  }

  // Function to insert Javadoc comments
  function insertJavadoc(node, javadoc) {
    const comment = {
      type: 'CommentBlock',
      value: javadoc.replace('/**', '').replace('*/', '').trim(),
    };
    if (!node.leadingComments) {
      node.leadingComments = [];
    }
    node.leadingComments.push(comment);
  }

  // Traverse the AST and insert Javadoc comments
  try {
    traverse(ast, {
      enter(path) {
        const node = path.node;
        if (node.type === 'FunctionDeclaration' || node.type === 'FunctionExpression') {
          const name = node.id ? node.id.name : 'anonymous';
          if (docstringsMapping[name]) {
            insertJavadoc(node, docstringsMapping[name]);
          }
        } else if (node.type === 'ClassDeclaration') {
          const className = node.id ? node.id.name : 'anonymous';
          if (docstringsMapping[className]) {
            insertJavadoc(node, docstringsMapping[className]);
          }
          if (node.body && node.body.body) {
            node.body.body.forEach(element => {
              if (element.type === 'MethodDefinition') {
                const methodName = element.key.name;
                const fullName = `${className}.${methodName}`;
                if (docstringsMapping[fullName]) {
                  insertJavadoc(element, docstringsMapping[fullName]);
                }
              }
            });
          }
        }
      },
    });
  } catch (e) {
    console.error(`Error traversing AST: ${e.message}`);
    process.exit(1);
  }

  // Generate code from modified AST, including comments
  let modifiedCode;
  try {
    const result = generate(ast, { comments: true });
    modifiedCode = result.code;
  } catch (e) {
    console.error(`Error generating code from AST: ${e.message}`);
    process.exit(1);
  }

  // Output the modified code
  console.log(modifiedCode);
});

```

