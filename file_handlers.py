mport os
import shutil
import logging
import aiofiles
import aiohttp
import json
import asyncio
from typing import Set, List, Dict, Any, Optional
from language_functions import get_handler
from language_functions.base_handler import BaseHandler
from utils import (
    is_binary,
    get_language,
    is_valid_extension,
    clean_unused_imports_async,
    format_with_black_async,
    run_flake8_async
)
from write_documentation_report import (
    generate_documentation_prompt,
    write_documentation_report,
    sanitize_filename,
    generate_table_of_contents
)

logger = logging.getLogger(__name__)

async def extract_code_structure(content: str, file_path: str, language: str, handler: BaseHandler) -> Optional[Dict[str, Any]]:
    """
    Asynchronously extracts the code structure from the given content using the specified handler.

    Args:
        content (str): The source code content.
        file_path (str): Path to the source file.
        language (str): Programming language of the source code.
        handler (BaseHandler): The handler object for the specific language.

    Returns:
        Optional[Dict[str, Any]]: A dictionary representing the code structure or None if extraction fails.
    """
    logger.debug(f"Extracting code structure for '{file_path}' (language: {language})")
    try:
        structure = await asyncio.to_thread(handler.extract_structure, content, file_path)
        if not structure:
            logger.warning(f"No structure extracted from '{file_path}'")
            return None
        return structure
    except Exception as e:
        logger.error(f"Error extracting structure from '{file_path}': {e}", exc_info=True)
        return None

async def backup_and_write_new_content(file_path: str, new_content: str) -> None:
    """
    Creates a backup of the original file and writes new content to it.

    Args:
        file_path (str): Path to the file to update.
        new_content (str): The new content to write to the file.
    """
    backup_path = f'{file_path}.bak'
    try:
        if os.path.exists(backup_path):
            os.remove(backup_path)
            logger.debug(f"Removed existing backup at '{backup_path}'.")
        await asyncio.to_thread(shutil.copy, file_path, backup_path)
        logger.debug(f"Backup created at '{backup_path}'.")
        async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
            await f.write(new_content)
        logger.info(f"Inserted documentation into '{file_path}'.")
    except Exception as e:
        logger.error(f"Error writing to '{file_path}': {e}", exc_info=True)
        # Attempt to restore from backup
        if os.path.exists(backup_path):
            try:
                await asyncio.to_thread(shutil.copy, backup_path, file_path)
                os.remove(backup_path)
                logger.info(f"Restored original file from backup for '{file_path}'.")
            except Exception as restore_error:
                logger.error(f"Failed to restore backup for '{file_path}': {restore_error}", exc_info=True)

async def fetch_documentation_rest(
    session: aiohttp.ClientSession,
    prompt: str,
    semaphore: asyncio.Semaphore,
    deployment_name: str,
    function_schema: Dict[str, Any],
    azure_api_key: str,
    azure_endpoint: str,
    azure_api_version: str,
    retry: int = 3
) -> Optional[Dict[str, Any]]:
    """
    Fetches documentation from the Azure OpenAI API using the provided prompt.

    Args:
        session (aiohttp.ClientSession): The HTTP session for making requests.
        prompt (str): The prompt to send to the API.
        semaphore (asyncio.Semaphore): Semaphore to limit concurrent API requests.
        deployment_name (str): The Azure OpenAI deployment name.
        function_schema (Dict[str, Any]): The schema defining functions.
        azure_api_key (str): The API key for Azure OpenAI.
        azure_endpoint (str): The endpoint URL for the Azure OpenAI service.
        azure_api_version (str): The API version to use.
        retry (int, optional): Number of retries for failed requests. Defaults to 3.

    Returns:
        Optional[Dict[str, Any]]: The documentation generated by the API or None if failed.
    """
    logger.debug(f"Fetching documentation using REST API for deployment: {deployment_name}")

    url = f"{azure_endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version={azure_api_version}"
    headers = {
        "Content-Type": "application/json",
        "api-key": azure_api_key,
    }

    for attempt in range(1, retry + 1):
        try:
            async with semaphore:
                async with session.post(url, headers=headers, json={
                    "messages": [{"role": "user", "content": prompt}],
                    "functions": function_schema["functions"],
                    "function_call": {"name": "generate_documentation"},
                }) as response:
                    if response.status == 200:
                        data = await response.json()
                        logger.debug(f"API Response: {data}")

                        if "choices" in data and len(data["choices"]) > 0:
                            choice = data["choices"][0]
                            message = choice["message"]

                            if "function_call" in message:
                                function_call = message["function_call"]
                                if function_call["name"] == "generate_documentation":
                                    arguments = function_call["arguments"]
                                    try:
                                        documentation = json.loads(arguments)
                                        logger.debug("Received documentation via function_call.")
                                        return documentation
                                    except json.JSONDecodeError as e:
                                        logger.error(f"Error decoding JSON from function_call arguments: {e}")
                                        logger.error(f"Arguments Content: {arguments}")
                                        continue
                            logger.error("No valid function_call found in the response.")
                        else:
                            logger.error("No choices found in the API response.")
                    else:
                        error_text = await response.text()
                        logger.error(f"Request failed with status {response.status}: {error_text}")

            if attempt < retry:
                wait_time = min(2 ** attempt, 16)
                logger.info(f"Retrying after {wait_time} seconds... (Attempt {attempt}/{retry})")
                await asyncio.sleep(wait_time)
        except Exception as e:
            logger.error(f"An unexpected error occurred during API request: {e}", exc_info=True)
            if attempt < retry:
                wait_time = min(2 ** attempt, 16)
                logger.info(f"Retrying after {wait_time} seconds... (Attempt {attempt}/{retry})")
                await asyncio.sleep(wait_time)

    logger.error("All retry attempts to fetch documentation failed.")
    return None

async def process_file(
    session: aiohttp.ClientSession,
    file_path: str,
    skip_types: Set[str],
    semaphore: asyncio.Semaphore,
    deployment_name: str,
    function_schema: Dict[str, Any],
    repo_root: str,
    project_info: str,
    style_guidelines: str,
    safe_mode: bool,
    azure_api_key: str,
    azure_endpoint: str,
    azure_api_version: str,
    output_dir: str
) -> Optional[str]:
    """
    Processes a single file to extract its structure and generate documentation.

    Args:
        session (aiohttp.ClientSession): The HTTP session for making requests.
        file_path (str): Path to the file to process.
        skip_types (Set[str]): Set of file extensions to skip.
        semaphore (asyncio.Semaphore): Semaphore to limit concurrent API requests.
        deployment_name (str): The Azure OpenAI deployment name.
        function_schema (Dict[str, Any]): The schema defining functions.
        repo_root (str): Root directory of the repository.
        project_info (str): Information about the project.
        style_guidelines (str): Documentation style guidelines.
        safe_mode (bool): If True, no files will be modified.
        azure_api_key (str): The API key for Azure OpenAI.
        azure_endpoint (str): The endpoint URL for the Azure OpenAI service.
        azure_api_version (str): The API version to use.
        output_dir (str): Directory to save documentation files.

    Returns:
        Optional[str]: The content of the documentation report or None if processing fails.
    """
    logger.debug(f'Processing file: {file_path}')
    try:
        _, ext = os.path.splitext(file_path)
        if not is_valid_extension(ext, skip_types) or is_binary(file_path):
            logger.debug(f"Skipping file '{file_path}' due to invalid extension or binary content.")
            return None

        language = get_language(ext)
        logger.debug(f"Detected language for '{file_path}': {language}")

        handler: Optional[BaseHandler] = get_handler(language, function_schema)
        if handler is None:
            logger.warning(f'Unsupported language: {language}')
            return None

        logger.info(f'Processing file: {file_path}')

        try:
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                content = await f.read()
            logger.debug(f"File content for '{file_path}' read successfully.")
        except Exception as e:
            logger.error(f"Failed to read '{file_path}': {e}", exc_info=True)
            return None

        documentation = None
        code_structure = None

        try:
            code_structure = await extract_code_structure(content, file_path, language, handler)
            if not code_structure:
                logger.warning(f"Could not extract code structure from '{file_path}'")
            else:
                logger.debug(f"Extracted code structure for '{file_path}': {code_structure}")
                prompt = generate_documentation_prompt(
                    file_name=os.path.basename(file_path),
                    code_structure=code_structure,
                    project_info=project_info,
                    style_guidelines=style_guidelines,
                    language=language,
                    function_schema=function_schema
                )
                documentation = await fetch_documentation_rest(
                    session=session,
                    prompt=prompt,
                    semaphore=semaphore,
                    deployment_name=deployment_name,
                    function_schema=function_schema,
                    azure_api_key=azure_api_key,
                    azure_endpoint=azure_endpoint,
                    azure_api_version=azure_api_version
                )
                if not documentation:
                    logger.error(f"Failed to generate documentation for '{file_path}'.")
                else:
                    # Combine code_structure with documentation as per schema
                    documentation['halstead'] = code_structure.get('halstead', {})
                    documentation['maintainability_index'] = code_structure.get('maintainability_index', None)
                    documentation['variables'] = code_structure.get('variables', [])
                    documentation['constants'] = code_structure.get('constants', [])
                    # Ensure 'changes_made' exists as per schema
                    documentation['changes_made'] = documentation.get('changes_made', [])
                    # Update functions and methods with complexity
                    function_complexity = {}
                    for func in code_structure.get('functions', []):
                        function_complexity[func['name']] = func.get('complexity', 0)
                    for func in documentation.get('functions', []):
                        func_name = func['name']
                        func['complexity'] = function_complexity.get(func_name, 0)
                    class_complexity = {}
                    for cls in code_structure.get('classes', []):
                        class_name = cls['name']
                        methods_complexity = {}
                        for method in cls.get('methods', []):
                            methods_complexity[method['name']] = method.get('complexity', 0)
                        class_complexity[class_name] = methods_complexity
                    for cls in documentation.get('classes', []):
                        class_name = cls['name']
                        methods_complexity = class_complexity.get(class_name, {})
                        for method in cls.get('methods', []):
                            method_name = method['name']
                            method['complexity'] = methods_complexity.get(method_name, 0)
        except Exception as e:
            logger.error(f"Error during code structure extraction or documentation generation for '{file_path}': {e}", exc_info=True)

        new_content = content

        if documentation and not safe_mode:
            try:
                new_content = await asyncio.to_thread(handler.insert_docstrings, content, documentation)

                if language.lower() == 'python':
                    new_content = await clean_unused_imports_async(new_content, file_path)
                    new_content = await format_with_black_async(new_content)

                is_valid = await asyncio.to_thread(handler.validate_code, new_content, file_path)
                if is_valid:
                    await backup_and_write_new_content(file_path, new_content)
                    logger.info(f"Documentation inserted into '{file_path}'")
                else:
                    logger.error(f"Code validation failed for '{file_path}'.")
            except Exception as e:
                logger.error(f"Error processing code documentation for '{file_path}': {e}", exc_info=True)
                new_content = content

        file_content = await write_documentation_report(
            documentation=documentation or {},
            language=language,
            file_path=file_path,
            repo_root=repo_root,
            output_dir=output_dir
        )
        logger.info(f"Finished processing '{file_path}'")
        return file_content

    except Exception as e:
        logger.error(f"Error processing file '{file_path}': {e}", exc_info=True)
        return None

async def process_all_files(
    session: aiohttp.ClientSession,
    file_paths: List[str],
    skip_types: Set[str],
    semaphore: asyncio.Semaphore,
    deployment_name: str,
    function_schema: Dict[str, Any],
    repo_root: str,
    project_info: Optional[str],
    style_guidelines: Optional[str],
    safe_mode: bool = False,
    output_file: str = 'output.md',
    azure_api_key: str = '',
    azure_endpoint: str = '',
    azure_api_version: str = '',
    output_dir: str = 'documentation'
) -> None:
    """
    Processes multiple files to extract their structures and generate documentation.

    Args:
        session (aiohttp.ClientSession): The HTTP session for making requests.
        file_paths (List[str]): List of file paths to process.
        skip_types (Set[str]): Set of file extensions to skip.
        semaphore (asyncio.Semaphore): Semaphore to limit concurrent API requests.
        deployment_name (str): The Azure OpenAI deployment name.
        function_schema (Dict[str, Any]): The schema defining functions.
        repo_root (str): Root directory of the repository.
        project_info (Optional[str]): Information about the project.
        style_guidelines (Optional[str]): Documentation style guidelines.
        safe_mode (bool, optional): If True, no files will be modified. Defaults to False.
        output_file (str, optional): Path to the output Markdown file. Defaults to 'output.md'.
        azure_api_key (str, optional): The API key for Azure OpenAI. Defaults to ''.
        azure_endpoint (str, optional): The endpoint URL for the Azure OpenAI service. Defaults to ''.
        azure_api_version (str, optional): The API version to use. Defaults to ''.
        output_dir (str, optional): Directory to save documentation files. Defaults to 'documentation'.
    """
    logger.info('Starting process of all files.')
    tasks = [
        process_file(
            session=session,
            file_path=file_path,
            skip_types=skip_types,
            semaphore=semaphore,
            deployment_name=deployment_name,
            function_schema=function_schema,
            repo_root=repo_root,
            project_info=project_info,
            style_guidelines=style_guidelines,
            safe_mode=safe_mode,
            azure_api_key=azure_api_key,
            azure_endpoint=azure_endpoint,
            azure_api_version=azure_api_version,
            output_dir=output_dir
        )
        for file_path in file_paths
    ]

    documentation_contents = []
    for f in asyncio.as_completed(tasks):
        try:
            file_content = await f
            if file_content:
                documentation_contents.append(file_content)
        except Exception as e:
            logger.error(f'Error processing a file: {e}', exc_info=True)
            if 'sentry_sdk' in globals():
                sentry_sdk.capture_exception(e)

    logger.info('Completed processing all files.')

    final_content = '\n\n'.join(documentation_contents)

    if final_content:
        toc = generate_table_of_contents(final_content)
        report_content = '# Documentation Generation Report\n\n## Table of Contents\n\n' + toc + '\n\n' + final_content

        try:
            async with aiofiles.open(output_file, 'w', encoding='utf-8') as f:
                await f.write(report_content)
            logger.info(f"Documentation report written to '{output_file}'")
        except Exception as e:
            logger.error(f"Error writing final documentation to '{output_file}': {e}", exc_info=True)
            if 'sentry_sdk' in globals():
                sentry_sdk.capture_exception(e)
    else:
        logger.warning("No documentation was generated.")

    logger.info('Running Flake8 on processed files for final linting.')
    for file_path in file_paths:
        _, ext = os.path.splitext(file_path)
        if ext.lower() in {'.py'}:
            flake8_output = await run_flake8_async(file_path)
            if flake8_output:
                logger.warning(f'Flake8 issues found in {file_path}:\n{flake8_output}')
    logger.info('Flake8 linting completed.')